{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMwhtzo1v+gs6LDwgtbc2Dt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LeonimerMelo/GANs/blob/main/Introduction_to_Generative_Adversarial_Networks_(GANs)_v6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Generative Adversarial Networks (GANs) - Redes Generativas Adversárias\n",
        "As GANs são um novo tipo de arquitetura neural introduzida por Ian Goodfellow e outros pesquisadores da Universidade de Montreal em 2014. A característica mais notável das GANs é sua capacidade de criar imagens, vídeos, músicas e textos hiper-realistas.  \n",
        "\n",
        "Esses modelos de rede neural realmente criam novos conteúdos que não existiam antes — novas pessoas, novos objetos, uma nova realidade, como mágica! Treinamos esses modelos em um conjunto de dados de um domínio específico, e então eles criam novas imagens com objetos do mesmo domínio que parecem próximos aos dados reais.  \n",
        "\n",
        "Veja exemplos de geradores de rostos com GANs no site [This Person Does Not Exist](https://thispersondoesnotexist.com/). Ele usa uma tecnologia especial chamada StyleGAN3, que funciona como um artista no mundo digital. Ela está aqui para criar rostos que parecem tão reais que você pode pensar que são pessoas de verdade — mas não são! Esses rostos são todos feitos por um computador superinteligente, usando suas habilidades para criar indivíduos detalhados e únicos que você não encontrará em nenhum outro lugar.  \n",
        "\n",
        "Ao aprender a imitar qualquer distribuição de dados, os computadores podem ser ensinados a criar mundos semelhantes ao nosso em qualquer domínio: imagens, música, fala, prosa, etc.  \n",
        "\n",
        "Uma das principais vantagens das GANs é sua capacidade de gerar novos dados, semelhantes aos dados de treinamento, sem modelar explicitamente a distribuição dos dados. Isso as torna ideais para tarefas de aumento e síntese de dados, onde são necessários dados adicionais para treinar modelos de aprendizado de máquina. As GANs também podem ser usadas para aprendizado não supervisionado com dados de treinamento não rotulados. No entanto, as GANs não estão sem problemas. Elas podem sofrer de instabilidade durante o treinamento, colapso de modos e falta de diversidade nos dados gerados. Várias técnicas foram propostas para resolver esses problemas, como o uso de funções de perda alternativas, regularização do gerador e incorporação de feedback do discriminador no gerador."
      ],
      "metadata": {
        "id": "Lq3bSFwhsvds"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center><img src='https://drive.google.com/uc?id=1ahdK9NNFOUbWhM6t6Zctpd9ulBVa1tc6' width=800></center>"
      ],
      "metadata": {
        "id": "zURzjA84tNNQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Em geral, as GANs são uma ferramenta poderosa para geração de dados e têm o potencial de revolucionar diversos campos, incluindo arte, design e entretenimento. A figura acima apresenta uma comparação visual convincente entre imagens originais e sintetizadas de rostos falantes, utilizando trilhas de pontos de referência faciais (landmarks). No lado esquerdo (a), vemos as imagens originais, que servem como ponto de referência. Em contraste, o lado direito (b) exibe as imagens geradas, criadas por meio de Redes Generativas Adversariais (GANs). A justaposição oferece uma visão clara das capacidades das GANs em criar representações realistas com base em landmarks faciais, destacando os avanços nas técnicas de síntese de imagens. Este capítulo revisará ainda mais a arquitetura geral das GANs, suas aplicações e os métodos mais conhecidos e populares."
      ],
      "metadata": {
        "id": "bZ-J-ticuFWX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introdução às GANs  \n",
        "As GANs são baseadas na ideia de *treinamento adversarial*. A arquitetura das GANs consiste basicamente em duas redes neurais que competem entre si:  \n",
        "- **O gerador** tenta converter ruído aleatório em observações que parecem ter sido amostradas do conjunto de dados original.  \n",
        "- **O discriminador** tenta prever se uma observação vem do conjunto de dados original ou se é uma das falsificações do gerador.  \n",
        "\n",
        "Essa competitividade ajuda as GANs a imitar qualquer distribuição de dados. Podemos pensar na arquitetura das GANs como dois boxeadores lutando: em sua busca para vencer a luta, ambos aprendem os movimentos e técnicas um do outro. Eles começam com menos conhecimento sobre o oponente e, à medida que a luta avança, aprendem e se tornam melhores."
      ],
      "metadata": {
        "id": "4_-usUg5CE6S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<figure>\n",
        "<center>\n",
        "<img src='https://drive.google.com/uc?id=1ZeCAtyVjyR1n5ACWtkgie3EBIa1-eqkC' />\n",
        "<figcaption>A fight between two adversarial networks: generative and discriminative</figcaption></center>\n",
        "</figure>"
      ],
      "metadata": {
        "id": "t8ZoaC5JLIBe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Modelos generativos utilizam aprendizado de máquina não supervisionado para descobrir e aprender automaticamente padrões em dados de entrada, o que lhes permite gerar novos exemplos que se assemelham aos do conjunto de dados original. A arquitetura das GANs é dividida em dois submodelos: o **modelo gerador**, que busca criar novos exemplos, e o **modelo discriminador**, que tem como objetivo classificar os exemplos como reais ou falsos. Esses dois modelos são treinados simultaneamente em um jogo adversarial de soma zero, até que o modelo discriminador seja enganado aproximadamente metade do tempo. Isso indica que o modelo gerador está produzindo instâncias convincentes."
      ],
      "metadata": {
        "id": "DBlc0vQkvR8u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Outra analogia pode ajudar a consolidar a ideia: pense em uma GAN como a oposição entre um falsificador e um policial em um jogo de gato e rato, onde o falsificador está aprendendo a passar notas falsas, e o policial está aprendendo a detectá-las. Ambos são dinâmicos: à medida que o falsificador aprende a aperfeiçoar a criação de notas falsas, o policial está em treinamento e se tornando melhor em detectar as falsificações. Cada lado aprende os métodos do outro em uma escalada constante."
      ],
      "metadata": {
        "id": "1es8LvutMU0m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<figure>\n",
        "<center>\n",
        "<img src='https://drive.google.com/uc?id=1VbDF_NwVuWUM9FfWznVyaPvXxUYBFxmH' width=900 />\n",
        "<figcaption>The GAN’s generator and discriminator models are like a counterfeiter and a police officer</figcaption></center>\n",
        "</figure>\n"
      ],
      "metadata": {
        "id": "YDBx67i_Mhct"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Arquitetura das GANs  \n",
        "Uma GAN consiste em dois componentes: o **Gerador** e o **Discriminador**. O Gerador é projetado para gerar dados consistentes com critérios de aceitabilidade, enquanto o Discriminador é treinado para distinguir entre dados falsos gerados pelo Gerador e dados reais. Se o Gerador produz resultados improváveis, o Discriminador o penaliza. Tanto o processo de geração quanto o de discriminação empregam redes neurais, com a saída do Gerador conectada diretamente à entrada do Discriminador. A classificação do Discriminador envia um sinal ao Gerador, levando-o a atualizar seus pesos por meio de retropropagação (backpropagation).  \n",
        "\n",
        "O diagrama de arquitetura de uma GAN segue os seguintes passos:  \n",
        "1. O gerador recebe números aleatórios e retorna uma imagem.  \n",
        "2. Essa imagem gerada é alimentada no discriminador junto com um fluxo de imagens extraídas do conjunto de dados real (ground truth).  \n",
        "3. O discriminador recebe tanto imagens reais quanto falsas e retorna probabilidades: números entre 0 e 1, onde 1 representa uma previsão de autenticidade e 0 representa uma previsão de falsificação."
      ],
      "metadata": {
        "id": "KCZjm2oONkGG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center><img src='https://drive.google.com/uc?id=18FoExFcG3DZhMfFt6ys4AsrqpWuq7jCR' width=900></center>"
      ],
      "metadata": {
        "id": "awvoiMt3OGRF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A arquitetura das GANs é composta pelas redes do gerador e do discriminador. Observe que a rede do discriminador é uma CNN (Rede Neural Convolucional) típica, na qual as camadas convolucionais reduzem de tamanho até chegarem à camada achatada (flattened layer). Por outro lado, a rede do gerador é uma CNN invertida que começa com o vetor achatado: as camadas convolucionais aumentam de tamanho até formarem a dimensão das imagens de entrada.\n"
      ],
      "metadata": {
        "id": "7bgUP5lLRUEM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vamos revisar o processo geral das GANs. Inicialmente, vetores de entrada aleatórios, como aqueles gerados por uma distribuição Gaussiana, são alimentados no Gerador para criar um modelo gerador. Esse modelo então gera novos exemplos, resultando em dois conjuntos: exemplos reais do banco de dados original e exemplos gerados pelo modelo gerador. O Discriminador usa ambos os conjuntos para classificá-los como reais ou falsos. Após essa classificação, o modelo gerador é atualizado com base na saída do Discriminador. Se o Discriminador não conseguir distinguir entre dados reais e falsos, a atualização dos parâmetros do modelo gerador e do modelo Discriminador pode ser pulada. A figura acima ilustra a arquitetura geral das GANs, onde as entradas são vetores aleatórios (por exemplo, ruído) seguindo uma distribuição definida, e a saída é classificada como dados reais ou falsos."
      ],
      "metadata": {
        "id": "PH2uXxwcxniG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## O modelo Discriminador  \n",
        "O objetivo do discriminador é prever se uma imagem é real ou falsa. Esse é um problema típico de classificação supervisionada, então podemos usar uma rede de classificação tradicional. A rede consiste em camadas convolucionais empilhadas, seguidas por uma camada densa de saída com uma função de ativação sigmoide. Usamos a função de ativação sigmoide porque esse é um problema de classificação binária: o objetivo da rede é gerar valores de probabilidade de previsão que variam entre 0 e 1, onde 0 significa que a imagem gerada pelo gerador é falsa e 1 significa que é 100% real."
      ],
      "metadata": {
        "id": "aBJx4f7aTGEa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center><img src='https://drive.google.com/uc?id=1lK04uhbNwlbuf2s7pOFFQO3jAOMGoXeI' width=900></center>"
      ],
      "metadata": {
        "id": "QuSXkgYqTz3l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "O discriminador é um modelo de classificação normal e bem compreendido. Treinar o discriminador é bastante direto. Alimentamos o discriminador com imagens rotuladas: falsas (ou geradas) e reais. As imagens reais vêm do conjunto de dados de treinamento, e as imagens falsas são a saída do modelo gerador.\n",
        "\n",
        "O resumo da saída do modelo discriminador, como você pode ter notado, não traz nada de novo: o modelo discriminador segue o padrão regular das redes CNN tradicionais. Empilhamos camadas convolucionais, de normalização em lote (batch normalization), de ativação e de dropout para criar nosso modelo. Todas essas camadas têm hiperparâmetros que ajustamos durante o treinamento da rede. Para sua própria implementação, você pode ajustar esses hiperparâmetros e adicionar ou remover camadas conforme achar necessário."
      ],
      "metadata": {
        "id": "h7rnYHUOUZEi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Em uma GAN, o discriminador pode ser visto como um classificador em vez de um discriminador tradicional. Seu principal objetivo é distinguir entre dados reais e dados gerados pelo gerador. A arquitetura do discriminador pode variar dependendo do tipo de dados que está categorizando, e ele pode utilizar qualquer arquitetura de rede adequada. Os dados de treinamento para o discriminador vêm de duas fontes: instâncias reais de dados, como fotografias de pessoas reais, e exemplos gerados pelo gerador. O discriminador usa dados reais como exemplos positivos durante o treinamento, tratando-os como bons pontos de referência para avaliar a qualidade dos dados falsos gerados. Durante essa fase, o gerador permanece inalterado, mantendo os mesmos pesos enquanto produz amostras para o discriminador praticar. O discriminador está conectado a duas funções de perda distintas: a perda do discriminador e a perda do gerador. Durante seu treinamento, o discriminador se concentra apenas na perda do discriminador, ignorando a perda do gerador. Por outro lado, o gerador usa a perda do gerador para seu treinamento. Se o discriminador classificar incorretamente uma instância real como falsa ou uma instância falsa como real, ele sofre uma perda devido ao seu erro. A rede do discriminador atualiza seus pesos por meio de retropropagação com base na perda do discriminador. Em outras palavras, o discriminador aprende com seus erros e ajusta seus parâmetros de acordo. Na prática, uma instância do domínio de entrada é alimentada no modelo discriminador, que então toma uma decisão de classificação atribuindo a ela um rótulo de classe verdadeiro ou falso, indicando se pertence à categoria de dados reais ou falsos."
      ],
      "metadata": {
        "id": "ic_6CC-oz9oX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## O modelo Gerador  \n",
        "O treinamento do gerador requer uma integração mais próxima entre o gerador e o discriminador em comparação com o treinamento do discriminador. Durante o treinamento de redes neurais, os pesos são ajustados para minimizar o erro ou a perda na saída. No framework das GANs, a perda do gerador penaliza o gerador por produzir amostras falsas, conforme avaliado pela rede discriminadora. O modelo gerador recebe um vetor aleatório de um comprimento predeterminado como entrada e gera uma amostra do domínio desejado. Para iniciar o processo generativo, um vetor é selecionado aleatoriamente de uma distribuição Gaussiana e usado como entrada.  \n",
        "\n",
        "O gerador recebe alguns dados aleatórios e tenta imitar o conjunto de dados de treinamento para gerar imagens falsas. Seu objetivo é enganar o discriminador, tentando gerar imagens que sejam réplicas perfeitas do conjunto de dados de treinamento. À medida que é treinado, ele melhora cada vez mais após cada iteração. No entanto, o discriminador também está sendo treinado ao mesmo tempo, então o gerador precisa continuar melhorando à medida que o discriminador aprende seus truques.  \n",
        "\n",
        "Como você pode ver na figura abaixo, o modelo gerador se parece com uma **ConvNet invertida**. O gerador recebe uma entrada vetorial com alguns dados de ruído aleatório e a remodela em um volume cúbico que tem largura, altura e profundidade. Esse volume deve ser tratado como um mapa de características que será alimentado em várias camadas convolucionais para criar a imagem final."
      ],
      "metadata": {
        "id": "bPYYRCaHWwjt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center><img src='https://drive.google.com/uc?id=1o5Y9cJfqGiQCPjxsXmDVDFmniiumZReL' width=800></center>"
      ],
      "metadata": {
        "id": "uEx_cvCeXgr7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Upsampling para dimensionar mapas de características  \n",
        "Redes neurais convolucionais tradicionais usam camadas de pooling para reduzir a dimensão das imagens de entrada. Para dimensionar os mapas de características, usamos **camadas de upsampling**, que aumentam as dimensões da imagem repetindo cada linha e coluna dos pixels de entrada.  \n",
        "\n",
        "O Keras possui uma camada de upsampling (`Upsampling2D`) que dimensiona as imagens ao receber um fator de escala (`size`) como argumento:  \n",
        "\n",
        "`keras.layers.UpSampling2D(size=(2, 2))`  \n",
        "\n",
        "Essa linha de código repete cada linha e coluna da matriz da imagem duas vezes, porque o fator de escala foi definido como (2, 2). Se o fator de escala for (3, 3), a camada de upsampling repetirá cada linha e coluna da matriz de entrada três vezes, como mostrado na figura."
      ],
      "metadata": {
        "id": "GQchUX6VYH4E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center><img src='https://drive.google.com/uc?id=1P9pW087aMGDIOSWpbaw3xlyDm2651LcY' width=700></center>"
      ],
      "metadata": {
        "id": "RmvqcOtLaHPk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## As Funções de Perda  \n",
        "Os métodos de perda discutidos aqui focam em derivar as perdas do gerador e do discriminador a partir de uma única medida de distância entre distribuições de probabilidade. Duas abordagens comuns são:  \n",
        "1. A função de perda minimax, que foi usada no artigo de pesquisa original que introduziu as GANs.  \n",
        "2. A função de perda de Wasserstein, inicialmente delineada em um estudo de 2017.  \n",
        "\n",
        "Essas abordagens representam maneiras diferentes de medir a discrepância entre as distribuições de dados gerados e reais. Sua eficácia e aplicabilidade dependem do problema específico e do conjunto de dados em questão. Pesquisas em andamento buscam explorar e refinar essas funções de perda e, potencialmente, introduzir novas para melhorar a estabilidade e o desempenho do treinamento das GANs."
      ],
      "metadata": {
        "id": "QSm-MxCf0t2Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Função de Perda Minimax das GANs  \n",
        "O treinamento das GANs é mais um **jogo de soma zero** do que um problema de otimização. Em jogos de soma zero, a pontuação total de utilidade é dividida entre os jogadores. Um aumento na pontuação de um jogador resulta em uma diminuição na pontuação de outro jogador. Na IA, isso é chamado de **teoria do jogo minimax**. Minimax é um algoritmo de tomada de decisão, normalmente usado em jogos de dois jogadores por turnos. O objetivo do algoritmo é encontrar a melhor jogada possível. Um jogador, chamado de maximizador, trabalha para obter a pontuação máxima possível; o outro jogador, chamado de minimizador, tenta obter a pontuação mais baixa, contra-atacando as jogadas do maximizador.  \n",
        "\n",
        "As GANs jogam um jogo minimax em que toda a rede tenta otimizar a função $V(D,G)$ na seguinte equação:\n",
        "\n",
        "$$\\min_G \\max_D V(D, G) = \\mathbb{E}_{x \\sim p_{data}(x)} [\\log D(x)] + \\mathbb{E}_{z \\sim p_z(z)} [\\log (1 - D(G(z)))]$$"
      ],
      "metadata": {
        "id": "qGmncMufOZ-W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center><img src='https://drive.google.com/uc?id=1HcxJJbWyxFqhnSnh9gG1tE18EZDFYJum' width=600></center>"
      ],
      "metadata": {
        "id": "lO5iy7H7PTky"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A equação acima representa a função de valor (ou função de custo) das GANs, onde:  \n",
        "- $D(x)$ é a saída do discriminador para uma amostra real $x$.  \n",
        "- $G(z)$ é a saída do gerador para uma amostra de ruído $z$.  \n",
        "- $p_{data}(x)$ é a distribuição dos dados reais.  \n",
        "- $p_z(z)$ é a distribuição do ruído de entrada (por exemplo, uma distribuição Gaussiana).  \n",
        "- $\\mathbb{E}$ denota o valor esperado.  \n",
        "\n",
        "O **discriminador (D)** tenta **maximizar** essa função, pois seu objetivo é distinguir corretamente entre dados reais ($x$) e dados falsos ($G(z)$). Ele quer maximizar $\\log D(x)$ para dados reais e $\\log (1 - D(G(z)))$ para dados falsos.  \n",
        "\n",
        "O **gerador (G)**, por outro lado, tenta **minimizar** essa função, pois seu objetivo é enganar o discriminador, fazendo com que $D(G(z))$ se aproxime de 1 (ou seja, o discriminador classifique as amostras geradas como reais). Ele quer minimizar $\\log (1 - D(G(z)))$.  \n",
        "\n",
        "Essa dinâmica de minimax cria um equilíbrio competitivo entre o gerador e o discriminador, onde ambos melhoram iterativamente até que o gerador produza amostras indistinguíveis dos dados reais."
      ],
      "metadata": {
        "id": "FsrU6cCywVu1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A função minimax é semelhante à **função de perda de entropia cruzada (cross-entropy loss)**, comumente aplicada em redes de aprendizado profundo (deep learning). Ambas as funções medem a discrepância entre duas distribuições de probabilidade: uma distribuição real e uma distribuição prevista (ou gerada).  \n",
        "\n",
        "No caso das GANs:  \n",
        "- O **discriminador** tenta maximizar a entropia cruzada entre as previsões para dados reais e falsos, ou seja, ele quer classificar corretamente as amostras.  \n",
        "- O **gerador** tenta minimizar a entropia cruzada entre as previsões do discriminador para as amostras geradas e o rótulo \"real\", ou seja, ele quer enganar o discriminador.  \n",
        "\n",
        "Essa relação faz com que a função minimax das GANs seja uma extensão natural da entropia cruzada, adaptada para o contexto adversarial em que o gerador e o discriminador competem entre si."
      ],
      "metadata": {
        "id": "pYa-ZQ2rQrTS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "O objetivo do discriminador (D) é *maximizar* a probabilidade de atribuir o rótulo correto à imagem. Por outro lado, o objetivo do gerador (G) é *minimizar* as chances de ser pego. Portanto, treinamos *D* para maximizar a probabilidade de atribuir o rótulo correto tanto aos exemplos de treinamento quanto às amostras geradas por *G*. Simultaneamente, treinamos *G* para minimizar $\\log(1 – D(G(z)))$. Em outras palavras, *D* e *G* jogam um jogo minimax de dois jogadores com a função de valor $V(D,G)$.  \n",
        "\n",
        "O discriminador recebe sua entrada de duas fontes:  \n",
        "- **Dados do gerador** $G(z)$ — Esses são dados falsos $(z)$. A saída do discriminador para os dados gerados é denotada como $D(G(z))$.  \n",
        "- **Entrada real dos dados de treinamento** $(x)$ — A saída do discriminador para os dados reais é denotada como $\\log D(x)$.  \n",
        "\n",
        "Para simplificar a equação minimax, a melhor maneira de analisá-la é dividi-la em dois componentes: a função de treinamento do discriminador e a função de treinamento do gerador (modelo combinado). Durante o processo de treinamento, criamos dois fluxos de treinamento, cada um com sua própria função de erro:  \n",
        "\n",
        "1. **Para o discriminador sozinho**, representado pela seguinte função, que visa maximizar a função minimax, fazendo as previsões o mais próximo possível de 1:  \n",
        "   $$E_{x \\sim p_{data}} [\\log D(x)]$$  \n",
        "\n",
        "2. **Para o modelo combinado** (treinamento do gerador), representado pela seguinte função, que visa minimizar a função minimax, fazendo as previsões o mais próximo possível de 0:  \n",
        "   $$E_{z \\sim P_z(z)} [\\log (1-D(G(z)))]$$  \n",
        "\n",
        "Agora que entendemos os símbolos da equação e temos uma compreensão melhor de como a função minimax funciona, vamos revisar a função novamente:  \n",
        "\n",
        "$$\\min_G \\max_D V(D, G) = \\mathbb{E}_{x \\sim p_{data}(x)} [\\log D(x)] + \\mathbb{E}_{z \\sim p_z(z)} [\\log (1 - D(G(z)))]$$  \n",
        "\n",
        "Essa equação encapsula a dinâmica competitiva entre o gerador e o discriminador, onde o gerador tenta minimizar a capacidade do discriminador de distinguir entre dados reais e falsos, enquanto o discriminador tenta maximizar essa capacidade. Essa interação leva ao aprimoramento iterativo de ambos os modelos até que o gerador produza amostras indistinguíveis dos dados reais."
      ],
      "metadata": {
        "id": "mYegUMccPsiE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center><img src='https://drive.google.com/uc?id=1-fmvmiuZMefwBe77o_IwbYuzAzULwOFk' width=600></center>"
      ],
      "metadata": {
        "id": "9Iw9tUYtU3SQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "O objetivo da função de custo minimax $V(D, G)$ é maximizar $D(x)$ para os dados reais da distribuição verdadeira e minimizar $D(G(z))$ para os dados falsos da distribuição gerada. Para alcançar isso, usamos a *log-verossimilhança* de $D(x)$ e $1 – D(G(z))$ na função objetivo. O logaritmo de um valor garante que, quanto mais próximo estivermos de um valor incorreto, mais seremos penalizados.  \n",
        "\n",
        "No início do processo de treinamento das GANs, o discriminador rejeitará os dados falsos do gerador com alta confiança, pois as imagens falsas são muito diferentes dos dados reais de treinamento — o gerador ainda não aprendeu. À medida que treinamos o discriminador para maximizar a probabilidade de atribuir os rótulos corretos tanto aos exemplos reais quanto às imagens falsas geradas pelo gerador, simultaneamente treinamos o gerador para minimizar o erro de classificação do discriminador para os dados falsos gerados.  \n",
        "\n",
        "O discriminador deseja maximizar os objetivos de forma que:  \n",
        "- $D(x)$ esteja próximo de 1 para dados reais.  \n",
        "- $D(G(z))$ esteja próximo de 0 para dados falsos.  \n",
        "\n",
        "Por outro lado, o gerador deseja minimizar os objetivos de forma que $D(G(z))$ esteja próximo de 1, para que o discriminador seja enganado, pensando que os dados gerados $G(z)$ são reais.  \n",
        "\n",
        "*Interrompemos o treinamento quando os dados falsos gerados pelo gerador são reconhecidos como dados reais*. Esse ponto de equilíbrio indica que o gerador aprendeu a produzir amostras convincentes e que o discriminador não consegue mais distinguir entre dados reais e falsos."
      ],
      "metadata": {
        "id": "ylmW-7lAVOPK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Função de Perda de Wasserstein  \n",
        "Essa função de perda é baseada na \"Wasserstein GAN\" ou \"WGAN\", uma variante do sistema GAN. Na WGAN, o discriminador não classifica instâncias, mas, em vez disso, retorna um valor numérico para cada instância. Como ele não diferencia entre conteúdo verdadeiro e falso, o discriminador da WGAN é mais precisamente chamado de \"crítico\" em vez de \"discriminador\".  \n",
        "\n",
        "A principal vantagem da WGAN é que ela resolve alguns problemas comuns no treinamento de GANs tradicionais, como instabilidade e colapso de modos. A função de perda de Wasserstein mede a distância entre as distribuições de dados reais e gerados de uma maneira mais suave e contínua, o que facilita o treinamento e melhora a qualidade das amostras geradas.  \n",
        "\n",
        "A função de perda de Wasserstein é definida como:  \n",
        "\n",
        "$$L = \\mathbb{E}_{x \\sim p_{data}} [D(x)] - \\mathbb{E}_{z \\sim p_z(z)} [D(G(z))]$$  \n",
        "\n",
        "Onde:  \n",
        "- $D(x)$ é a saída do crítico para dados reais.  \n",
        "- $D(G(z))$ é a saída do crítico para dados gerados.  \n",
        "\n",
        "O objetivo do crítico é maximizar essa função, enquanto o gerador tenta minimizá-la. A WGAN também introduz a restrição de Lipschitz no crítico, garantindo que ele seja uma função suave e estável. Isso é frequentemente implementado usando o método de **clipping de pesos** ou **gradient penalty** (penalidade de gradiente).  \n",
        "\n",
        "A WGAN é uma abordagem poderosa para melhorar a estabilidade e a qualidade do treinamento de GANs, especialmente em tarefas que exigem geração de dados complexos e de alta qualidade."
      ],
      "metadata": {
        "id": "VA7t03R91xlo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Treinando a GAN  \n",
        "Agora que aprendemos sobre os modelos do discriminador e do gerador separadamente, vamos juntá-los para treinar uma rede generativa adversarial de ponta a ponta. O discriminador está sendo treinado para se tornar um classificador melhor, maximizando a probabilidade de atribuir o rótulo correto tanto aos exemplos de treinamento (**reais**) quanto às imagens geradas pelo gerador (**falsas**): por exemplo, o policial se torna melhor em diferenciar entre moeda falsa e real. O gerador, por outro lado, está sendo treinado para se tornar um falsificador melhor, maximizando suas chances de enganar o discriminador. Ambas as redes estão melhorando em suas respectivas tarefas.  \n",
        "\n",
        "O processo de treinamento dos modelos GAN envolve dois processos:  \n",
        "\n",
        "1. **Treinar o discriminador**. Este é um processo de treinamento supervisionado direto. A rede recebe imagens rotuladas vindas do gerador (falsas) e dos dados de treinamento (reais), e aprende a classificar entre imagens reais e falsas com uma saída de previsão sigmoide. Nada de novo aqui.  \n",
        "2. **Treinar o gerador**. Este processo é um pouco mais complexo. O modelo gerador não pode ser treinado sozinho como o discriminador. Ele precisa do modelo discriminador para dizer se fez um bom trabalho ao falsificar imagens. Portanto, criamos uma rede combinada para treinar o gerador, composta por ambos os modelos: discriminador e gerador.  \n",
        "\n",
        "Pense nos processos de treinamento como duas pistas paralelas. Uma pista treina o discriminador sozinho, e a outra pista é o modelo combinado que treina o gerador. O processo de treinamento da GAN é ilustrado na figura.  \n",
        "\n",
        "### Fluxo de Treinamento:  \n",
        "1. **Treinamento do Discriminador**:  \n",
        "   - Alimente o discriminador com um lote de imagens reais e rotule-as como \"1\" (reais).  \n",
        "   - Gere um lote de imagens falsas usando o gerador e rotule-as como \"0\" (falsas).  \n",
        "   - Treine o discriminador para classificar corretamente as imagens reais e falsas.  \n",
        "\n",
        "2. **Treinamento do Gerador**:  \n",
        "   - Gere um lote de imagens falsas usando o gerador.  \n",
        "   - Conecte o gerador ao discriminador (rede combinada).  \n",
        "   - Treine o gerador para maximizar a probabilidade de o discriminador classificar as imagens falsas como reais.  \n",
        "\n",
        "Esse processo é repetido iterativamente até que o gerador produza imagens convincentes e o discriminador não consiga mais distinguir entre imagens reais e falsas. O equilíbrio entre os dois modelos é alcançado quando a GAN atinge seu ponto ótimo."
      ],
      "metadata": {
        "id": "iHVhpwp-ayjz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center><img src='https://drive.google.com/uc?id=1HxuZmA59YsfFBz8dwRo9AFBGkpBzuG3f' width=900></center>"
      ],
      "metadata": {
        "id": "eFF8Tsxkb48K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A imagem acima descreve o processo de treinamento de uma GAN (Generative Adversarial Network).\n",
        "\n",
        "### Treinamento do Discriminador:\n",
        "- **Dados Falsos do Gerador**: O gerador cria dados falsos.\n",
        "- **Dados Reais**: Dados reais do conjunto de treinamento.\n",
        "- **Atualização do Modelo**: O discriminador é atualizado para classificar corretamente os dados como reais ou falsos.\n",
        "- **Classificação Binária**: O discriminador realiza uma classificação binária (real/fake).\n",
        "\n",
        "### Treinamento do Gerador:\n",
        "- **Vetor de Entrada**: O gerador recebe um vetor de entrada (geralmente ruído aleatório).\n",
        "- **Atualização dos Pesos**: O gerador é treinado para melhorar a qualidade dos dados falsos.\n",
        "- **Dados Falsos e Reais**: O gerador tenta enganar o discriminador, que está congelado durante essa fase.\n",
        "- **Classificação Binária**: O discriminador ainda classifica os dados como reais ou falsos, mas seu objetivo é ser enganado pelo gerador.\n",
        "\n",
        "Esse processo iterativo ajuda a melhorar tanto o gerador quanto o discriminador, até que o gerador produza dados indistinguíveis dos reais."
      ],
      "metadata": {
        "id": "H6f5hP8VQkaw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Como você pode ver na figura, ao treinar o modelo combinado, congelamos os pesos do discriminador porque esse modelo se concentra apenas no treinamento do gerador.  \n",
        "\n",
        "Ambos os processos seguem o processo tradicional de treinamento de redes neurais. Começa com o processo de **feedforward** (propagação direta), faz previsões, calcula o erro e, em seguida, realiza a **retropropagação** (backpropagation) do erro.  \n",
        "\n",
        "- **No treinamento do discriminador**, o erro é retropropagado de volta ao modelo do discriminador para atualizar seus pesos.  \n",
        "- **No modelo combinado**, o erro é retropropagado de volta ao gerador para atualizar seus pesos.  \n",
        "\n",
        "Durante as iterações de treinamento, seguimos o mesmo procedimento de treinamento de redes neurais para observar o desempenho da rede e ajustar seus hiperparâmetros até que o gerador alcance resultados satisfatórios para o problema em questão. Esse é o momento em que podemos interromper o treinamento e implantar o modelo gerador.  \n",
        "\n",
        "Esse processo iterativo garante que o gerador melhore continuamente sua capacidade de gerar dados convincentes, enquanto o discriminador se torna cada vez mais difícil de enganar. O equilíbrio entre os dois modelos é alcançado quando o gerador produz dados indistinguíveis dos reais, e o discriminador não consegue mais diferenciar entre eles."
      ],
      "metadata": {
        "id": "hSl8dJYNcT_a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para aprender a distinguir entre dados reais e falsos, o treinamento do discriminador deve inicialmente se concentrar em identificar os erros cometidos pelo gerador. Isso apresenta um desafio único, especialmente para um gerador bem treinado em comparação com um que produz saídas aleatórias sem qualquer treinamento. Durante o processo de treinamento de uma GAN, o discriminador permanece inalterado enquanto o gerador é treinado, visando atingir um objetivo específico. O método de previsão do discriminador pode ser comparado ao lançamento de uma moeda e, idealmente, com um gerador bem treinado, a precisão do discriminador deve ser em torno de 50%. No entanto, à medida que o treinamento avança, o feedback do discriminador se torna menos significativo, o que representa um desafio para a convergência geral da GAN. O treinamento de uma GAN geralmente envolve três etapas, conforme mostrado na figura abaixo. A entrada inicial para o gerador são dados aleatórios, que não fazem parte do processo de treinamento. Como mencionado anteriormente, a função de perda é usada para o treinamento, e o treinamento inicial é realizado no discriminador, que então atualiza o gerador com base em suas saídas. Esse processo é repetido iterativamente até que o discriminador atinja um ponto em que não consegue distinguir entre dados reais e falsos. Nesse estágio, o gerador é considerado avaliado e testado. Ao passar por essas etapas de treinamento e iterações, uma GAN visa alcançar um estado em que os dados gerados se tornam altamente convincentes e indistinguíveis dos dados reais."
      ],
      "metadata": {
        "id": "ezkAjbDI3Jlc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center><img src='https://drive.google.com/uc?id=1blgfmU4ms8ctehXpOk32q8F5bugGe2LR' width=800></center>"
      ],
      "metadata": {
        "id": "68xmDiBy3diY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Treinando o Discriminador  \n",
        "Agora, vamos ver como compilamos as redes do discriminador e do modelo combinado para treinar o modelo GAN. Como mencionado anteriormente, esse é um processo direto. Primeiro, construímos o modelo a partir do método `discriminator_model`. Em seguida, compilamos o modelo usando a função de perda *binary_crossentropy* e um otimizador de sua escolha (usamos Adam neste exemplo).  \n",
        "\n",
        "Aqui está a implementação em Keras que constrói e compila o discriminador. Observe que este trecho de código não é compilável por si só — ele está aqui para ilustração:\n",
        "\n",
        "```python\n",
        "discriminator = discriminator_model()\n",
        "discriminator.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "```\n",
        "\n",
        "#### Explicação:\n",
        "1. **`discriminator_model()`**: Este método define a arquitetura do discriminador, que geralmente consiste em camadas convolucionais para processar imagens e uma camada densa final com ativação sigmoide para classificação binária (real/fake).  \n",
        "2. **`compile()`**: Compila o modelo, especificando:  \n",
        "   - **`loss='binary_crossentropy'`**: A função de perda usada para treinar o discriminador, adequada para problemas de classificação binária.  \n",
        "   - **`optimizer='adam'`**: O otimizador Adam é usado para ajustar os pesos do modelo durante o treinamento.  \n",
        "   - **`metrics=['accuracy']`**: A métrica de acurácia é usada para monitorar o desempenho do discriminador durante o treinamento.  \n",
        "\n",
        "Esse código prepara o discriminador para o treinamento, onde ele aprenderá a distinguir entre dados reais e falsos gerados pelo gerador."
      ],
      "metadata": {
        "id": "w4kRpVho48-R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Podemos treinar o modelo criando lotes de treinamento aleatórios usando o método `train_on_batch` do Keras, que executa uma única atualização de gradiente em um único lote de dados. Aqui está um exemplo de como isso pode ser feito:\n",
        "\n",
        "```python\n",
        "# Gera ruído aleatório\n",
        "noise = np.random.normal(0, 1, (batch_size, 100))\n",
        "\n",
        "# Gera um lote de novas imagens\n",
        "gen_imgs = generator.predict(noise)\n",
        "\n",
        "# Treina o discriminador (dados reais classificados como 1 e dados gerados como 0)\n",
        "d_loss_real = discriminator.train_on_batch(imgs, valid)\n",
        "d_loss_fake = discriminator.train_on_batch(gen_imgs, fake)\n",
        "```\n",
        "\n",
        "### Explicação:\n",
        "1. **Geração de Ruído**:  \n",
        "   - `noise = np.random.normal(0, 1, (batch_size, 100))`: Gera um vetor de ruído aleatório com distribuição normal (média 0 e desvio padrão 1) para alimentar o gerador. O tamanho do lote (`batch_size`) e a dimensão do ruído (100) são especificados.\n",
        "\n",
        "2. **Geração de Imagens Falsas**:  \n",
        "   - `gen_imgs = generator.predict(noise)`: O gerador usa o ruído para gerar um lote de imagens falsas.\n",
        "\n",
        "3. **Treinamento do Discriminador**:  \n",
        "   - `d_loss_real = discriminator.train_on_batch(imgs, valid)`: Treina o discriminador com um lote de imagens reais (`imgs`), onde os rótulos (`valid`) são 1 (indicando que as imagens são reais).  \n",
        "   - `d_loss_fake = discriminator.train_on_batch(gen_imgs, fake)`: Treina o discriminador com um lote de imagens falsas (`gen_imgs`), onde os rótulos (`fake`) são 0 (indicando que as imagens são falsas).  \n",
        "\n",
        "### Resultados:\n",
        "- `d_loss_real` e `d_loss_fake` armazenam a perda e a acurácia do discriminador para os lotes de dados reais e falsos, respectivamente. Esses valores podem ser usados para monitorar o desempenho do discriminador durante o treinamento.\n",
        "\n",
        "Esse processo é repetido iterativamente para cada lote de dados, permitindo que o discriminador melhore sua capacidade de distinguir entre dados reais e falsos, enquanto o gerador aprende a produzir imagens mais convincentes."
      ],
      "metadata": {
        "id": "4KXrUd0t6B-4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Treinando o Gerador (modelo combinado)  \n",
        "Aqui está a parte mais desafiadora do treinamento de GANs: o treinamento do gerador. Enquanto o discriminador pode ser treinado isoladamente, o gerador precisa do discriminador para ser treinado. Para isso, construímos um **modelo combinado** que inclui tanto o gerador quanto o discriminador, conforme ilustrado na figura abaixo.  \n",
        "\n",
        "#### Passos para Treinar o Gerador:\n",
        "1. **Congelar o Discriminador**:  \n",
        "   Durante o treinamento do gerador, os pesos do discriminador são congelados para evitar que ele seja atualizado. Isso garante que o gerador seja treinado com base nas previsões atuais do discriminador.  \n",
        "\n",
        "2. **Criar o Modelo Combinado**:  \n",
        "   O modelo combinado conecta o gerador ao discriminador. A entrada do modelo combinado é o ruído aleatório, e a saída é a previsão do discriminador sobre as imagens geradas.  \n",
        "\n",
        "3. **Compilar o Modelo Combinado**:  \n",
        "   O modelo combinado é compilado com uma função de perda que penaliza o gerador quando o discriminador classifica as imagens geradas como falsas. O objetivo do gerador é minimizar essa perda, fazendo com que o discriminador classifique as imagens geradas como reais.  \n",
        "\n",
        "Aqui está um exemplo de implementação em Keras:\n",
        "\n",
        "```python\n",
        "# Congela os pesos do discriminador no modelo combinado\n",
        "discriminator.trainable = False\n",
        "\n",
        "# Cria o modelo combinado\n",
        "combined_input = Input(shape=(100,))  # Entrada de ruído\n",
        "generated_img = generator(combined_input)  # Gera uma imagem\n",
        "combined_output = discriminator(generated_img)  # Classifica a imagem gerada\n",
        "combined_model = Model(combined_input, combined_output)\n",
        "\n",
        "# Compila o modelo combinado\n",
        "combined_model.compile(loss='binary_crossentropy', optimizer='adam')\n",
        "```\n",
        "\n",
        "#### Treinamento do Modelo Combinado:\n",
        "```python\n",
        "# Gera ruído aleatório\n",
        "noise = np.random.normal(0, 1, (batch_size, 100))\n",
        "\n",
        "# Rótulos \"reais\" para enganar o discriminador\n",
        "valid_y = np.ones((batch_size, 1))\n",
        "\n",
        "# Treina o gerador (via modelo combinado)\n",
        "g_loss = combined_model.train_on_batch(noise, valid_y)\n",
        "```\n",
        "\n",
        "### Explicação:\n",
        "1. **Congelar o Discriminador**:  \n",
        "   - `discriminator.trainable = False`: Garante que os pesos do discriminador não sejam atualizados durante o treinamento do gerador.  \n",
        "\n",
        "2. **Modelo Combinado**:  \n",
        "   - O gerador recebe ruído aleatório e gera uma imagem.  \n",
        "   - O discriminador classifica a imagem gerada.  \n",
        "   - O modelo combinado conecta o gerador ao discriminador.  \n",
        "\n",
        "3. **Compilação**:  \n",
        "   - A função de perda `binary_crossentropy` é usada para treinar o gerador.  \n",
        "   - O otimizador Adam ajusta os pesos do gerador.  \n",
        "\n",
        "4. **Treinamento**:  \n",
        "   - O gerador é treinado para maximizar a probabilidade de o discriminador classificar as imagens geradas como reais.  \n",
        "   - `g_loss` armazena a perda do gerador, que pode ser usada para monitorar o desempenho.  \n",
        "\n",
        "Esse processo iterativo permite que o gerador melhore continuamente sua capacidade de gerar imagens realistas, enquanto o discriminador se torna cada vez mais difícil de enganar. O equilíbrio entre os dois modelos é alcançado quando o gerador produz imagens indistinguíveis das reais."
      ],
      "metadata": {
        "id": "0KkG22yw60IE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center><img src='https://drive.google.com/uc?id=1Vz_lB46beZOIh9L_GGjshbrHb-IpcRCb' width=900></center>"
      ],
      "metadata": {
        "id": "FgsC8-rT7ShL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A imagem acima descreve o fluxo de feedback em uma GAN (Generative Adversarial Network).\n",
        "\n",
        "### Fluxo de Feedback em uma GAN:\n",
        "1. **Ruído Aleatório**: O gerador recebe um vetor de ruído aleatório como entrada. Esse ruído é geralmente gerado a partir de uma distribuição normal ou uniforme.\n",
        "\n",
        "2. **Gerador**: O gerador usa o ruído aleatório para gerar uma imagem falsa. O objetivo do gerador é produzir imagens que sejam indistinguíveis das imagens reais.\n",
        "\n",
        "3. **Discriminador**: A imagem gerada pelo gerador é então alimentada no discriminador, junto com imagens reais. O discriminador tenta classificar as imagens como reais ou falsas.\n",
        "\n",
        "4. **Saída**: O discriminador produz uma saída que indica a probabilidade de a imagem ser real ou falsa.\n",
        "\n",
        "5. **Feedback através da Retropropagação**: O feedback do discriminador é retropropagado para o gerador. Esse feedback é usado para ajustar os pesos do gerador, melhorando sua capacidade de gerar imagens realistas.\n",
        "\n",
        "Esse processo iterativo de geração, classificação e ajuste contínuo permite que o gerador e o discriminador melhorem suas habilidades ao longo do tempo, até que o gerador seja capaz de produzir imagens altamente convincentes."
      ],
      "metadata": {
        "id": "jt9TD17eSaLX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Quando queremos treinar o gerador, **congelamos** os pesos do modelo discriminador porque o gerador e o discriminador têm funções de perda diferentes que puxam em direções opostas. Se não congelarmos os pesos do discriminador, ele será puxado na mesma direção em que o gerador está aprendendo, tornando-o mais propenso a prever imagens geradas como reais, o que não é o resultado desejado. Congelar os pesos do modelo discriminador não afeta o modelo discriminador existente que compilamos anteriormente ao treinar o discriminador. Pense nisso como ter dois modelos discriminadores — não é o caso, mas é mais fácil de imaginar.\n",
        "\n",
        "Agora, vamos construir o modelo combinado:\n",
        "\n",
        "```python\n",
        "# Constrói o gerador\n",
        "generator = generator_model()\n",
        "\n",
        "# O gerador recebe ruído como entrada e gera uma imagem.\n",
        "z = Input(shape=(100,))\n",
        "image = generator(z)\n",
        "\n",
        "# Congela os pesos do modelo discriminador\n",
        "discriminator.trainable = False\n",
        "\n",
        "# O discriminador recebe as imagens geradas como entrada e determina sua validade.\n",
        "valid = discriminator(image)\n",
        "\n",
        "# O modelo combinado (gerador e discriminador empilhados) treina o gerador para enganar o discriminador.\n",
        "combined = Model(z, valid)\n",
        "```\n",
        "\n",
        "### Explicação:\n",
        "1. **Construção do Gerador**:  \n",
        "   - `generator = generator_model()`: Define a arquitetura do gerador, que recebe um vetor de ruído e gera uma imagem.\n",
        "\n",
        "2. **Entrada de Ruído**:  \n",
        "   - `z = Input(shape=(100,))`: Define a entrada do gerador como um vetor de ruído de dimensão 100.\n",
        "\n",
        "3. **Geração de Imagens**:  \n",
        "   - `image = generator(z)`: O gerador transforma o ruído em uma imagem.\n",
        "\n",
        "4. **Congelamento do Discriminador**:  \n",
        "   - `discriminator.trainable = False`: Congela os pesos do discriminador para evitar que ele seja atualizado durante o treinamento do gerador.\n",
        "\n",
        "5. **Classificação pelo Discriminador**:  \n",
        "   - `valid = discriminator(image)`: O discriminador classifica a imagem gerada como real ou falsa.\n",
        "\n",
        "6. **Modelo Combinado**:  \n",
        "   - `combined = Model(z, valid)`: Cria um modelo combinado que conecta o gerador ao discriminador. A entrada é o ruído, e a saída é a previsão do discriminador.\n",
        "\n",
        "Esse modelo combinado é usado para treinar o gerador, ajustando seus pesos para minimizar a capacidade do discriminador de distinguir entre imagens reais e falsas. O objetivo é que o gerador aprenda a produzir imagens cada vez mais realistas."
      ],
      "metadata": {
        "id": "29WBQ3DrS4ed"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Agora que construímos o modelo combinado, podemos prosseguir com o processo de treinamento normalmente. Compilamos o modelo combinado com a função de perda *binary_crossentropy* e o otimizador *Adam*:\n",
        "\n",
        "```python\n",
        "combined.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
        "\n",
        "# Treina o gerador (objetivo: fazer o discriminador classificar as imagens geradas como reais)\n",
        "g_loss = combined.train_on_batch(noise, valid)\n",
        "```\n",
        "\n",
        "### Explicação:\n",
        "1. **Compilação do Modelo Combinado**:  \n",
        "   - `combined.compile(loss='binary_crossentropy', optimizer=optimizer)`: Compila o modelo combinado com:  \n",
        "     - **Função de Perda**: `binary_crossentropy`, que mede a discrepância entre as previsões do discriminador e os rótulos desejados (neste caso, o gerador quer que o discriminador classifique as imagens geradas como reais, ou seja, rótulo 1).  \n",
        "     - **Otimizador**: `optimizer` (Adam, no exemplo), que ajusta os pesos do gerador durante o treinamento.  \n",
        "\n",
        "2. **Treinamento do Gerador**:  \n",
        "   - `g_loss = combined.train_on_batch(noise, valid)`: Treina o gerador em um único lote de dados.  \n",
        "     - **Entrada**: `noise` (ruído aleatório usado para gerar as imagens).  \n",
        "     - **Rótulos**: `valid` (um vetor de 1s, indicando que as imagens geradas devem ser classificadas como reais pelo discriminador).  \n",
        "     - **Saída**: `g_loss` (a perda do gerador, que pode ser usada para monitorar o desempenho).  \n",
        "\n",
        "### Objetivo do Treinamento:\n",
        "O gerador é treinado para minimizar a perda (`binary_crossentropy`), fazendo com que o discriminador classifique as imagens geradas como reais. Isso é alcançado ajustando os pesos do gerador para produzir imagens cada vez mais realistas.  \n",
        "\n",
        "Esse processo é repetido iterativamente, alternando entre o treinamento do discriminador e o treinamento do gerador, até que o gerador produza imagens convincentes e o discriminador não consiga mais distinguir entre imagens reais e falsas."
      ],
      "metadata": {
        "id": "uqXJfdlV9RXS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Épocas de Treinamento  \n",
        "Para cada época, os dois modelos compilados (discriminador e combinado) são treinados simultaneamente. Durante o processo de treinamento, tanto o gerador quanto o discriminador melhoram. Você pode observar o desempenho da sua GAN imprimindo os resultados após cada época (ou um conjunto de épocas) para ver como o gerador está se saindo na geração de imagens sintéticas. A figura mostra um exemplo da evolução do desempenho do gerador ao longo do processo de treinamento no conjunto de dados MNIST de dígitos.\n",
        "\n",
        "### Explicação:\n",
        "1. **Treinamento Simultâneo**:  \n",
        "   - O **discriminador** é treinado para distinguir entre imagens reais e falsas.  \n",
        "   - O **gerador** (através do modelo combinado) é treinado para enganar o discriminador, produzindo imagens cada vez mais realistas.  \n",
        "\n",
        "2. **Monitoramento do Desempenho**:  \n",
        "   - Após cada época (ou um conjunto de épocas), você pode imprimir ou visualizar as imagens geradas pelo gerador para avaliar seu progresso.  \n",
        "   - Isso permite que você observe como as imagens sintéticas evoluem ao longo do tempo, começando com ruído aleatório e gradualmente se tornando mais semelhantes aos dados reais.  \n",
        "\n",
        "3. **Exemplo com MNIST**:  \n",
        "   - No caso do conjunto de dados MNIST (dígitos manuscritos), você pode ver o gerador começando com imagens borradas e, ao longo das épocas, produzindo dígitos cada vez mais claros e reconhecíveis.  \n",
        "\n",
        "### Fluxo de Treinamento por Época:\n",
        "1. **Treinar o Discriminador**:  \n",
        "   - Alimente o discriminador com um lote de imagens reais e falsas.  \n",
        "   - Atualize os pesos do discriminador para melhorar sua capacidade de classificação.  \n",
        "\n",
        "2. **Treinar o Gerador**:  \n",
        "   - Gere um lote de imagens falsas usando o gerador.  \n",
        "   - Use o modelo combinado para treinar o gerador, ajustando seus pesos para enganar o discriminador.  \n",
        "\n",
        "3. **Avaliação**:  \n",
        "   - Após cada época, avalie o desempenho do gerador visualizando as imagens geradas.  \n",
        "   - Ajuste hiperparâmetros, se necessário, para melhorar o treinamento.  \n",
        "\n",
        "Esse processo iterativo continua até que o gerador produza imagens de alta qualidade e o discriminador não consiga mais distinguir entre dados reais e falsos. A figura mencionada ilustra essa evolução, mostrando como as imagens geradas melhoram ao longo do tempo."
      ],
      "metadata": {
        "id": "gvjPIwwJMv8N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center><img src='https://drive.google.com/uc?id=1prVlo0eugSmGU_16F1ponuy7AVIEwxV3' width=900></center>"
      ],
      "metadata": {
        "id": "RUJA5zF1Nwac"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "No exemplo fornecido, o arquivo `image.png` mostra a evolução do desempenho do gerador ao longo de várias épocas de treinamento de uma GAN. Aqui está uma explicação do conteúdo:\n",
        "\n",
        "### Evolução do Gerador ao Longo das Épocas:\n",
        "1. **Época 0**:  \n",
        "   - O gerador começa com dados de ruído aleatório, que ainda não representam as características do conjunto de dados de treinamento. As imagens geradas são borradas e não reconhecíveis.\n",
        "\n",
        "2. **Época 1,500**:  \n",
        "   - Após 1.500 épocas, o gerador começa a mostrar sinais de aprendizado, produzindo imagens que começam a se assemelhar vagamente aos dados reais, mas ainda são de baixa qualidade.\n",
        "\n",
        "3. **Época 2,500**:  \n",
        "   - Com 2.500 épocas, as imagens geradas mostram uma melhoria significativa, com características mais definidas, mas ainda podem apresentar imperfeições.\n",
        "\n",
        "4. **Época 3,500**:  \n",
        "   - Após 3.500 épocas, o gerador produz imagens de qualidade ainda melhor, com detalhes mais claros e uma aparência mais próxima dos dados reais.\n",
        "\n",
        "5. **Época 5,500**:  \n",
        "   - Com 5.500 épocas, as imagens geradas são bastante convincentes, com características bem definidas e uma aparência realista.\n",
        "\n",
        "6. **Época 7,500**:  \n",
        "   - Após 7.500 épocas, o gerador atinge um alto nível de qualidade, produzindo imagens que são difíceis de distinguir das reais.\n",
        "\n",
        "7. **Época 9,500**:  \n",
        "   - Com 9.500 épocas, o gerador produz imagens de alta qualidade que são quase indistinguíveis dos dados reais, demonstrando que o modelo atingiu um bom equilíbrio.\n",
        "\n",
        "### Observação Manual do Desempenho:\n",
        "- **Avaliação Visual**: Observar manualmente as imagens geradas pelo gerador é uma maneira eficaz de avaliar o desempenho do sistema. Isso ajuda a decidir o número adequado de épocas e quando parar o treinamento.\n",
        "- **Decisão de Parada**: O treinamento pode ser interrompido quando as imagens geradas atingirem uma qualidade satisfatória e o discriminador não conseguir mais distinguir entre dados reais e falsos.\n",
        "\n",
        "Esse processo de evolução ilustra como o gerador melhora gradualmente sua capacidade de gerar imagens realistas ao longo do treinamento, destacando a importância de monitorar o progresso para garantir resultados ótimos."
      ],
      "metadata": {
        "id": "HPKn14R4OGSb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Aplicações Populares das GANs  \n",
        "A modelagem generativa percorreu um longo caminho nos últimos cinco anos. O campo evoluiu a ponto de se esperar que a próxima geração de modelos generativos seja mais habilidosa em criar arte do que os humanos. As GANs agora têm o poder de resolver problemas em indústrias como saúde, automotiva, belas artes e muitas outras. Nesta seção, aprenderemos sobre alguns casos de uso de redes adversariais e qual arquitetura GAN é usada para cada aplicação. O objetivo desta seção não é implementar as variações da rede GAN, mas fornecer uma visão geral das aplicações potenciais dos modelos GAN e recursos para leitura adicional.\n",
        "\n",
        "As GANs têm inúmeras aplicações, sendo a mais emocionante o uso de GANs para tarefas que exigem a criação ou conceitos de novos exemplos. A capacidade de criar pinturas, desenhos e outras formas de expressão visual únicas e criativas. A conversão de imagens de um formato para outro pode transformar fotos em diferentes estados, como de dia para noite, de verão para inverno, e assim por diante. As GANs criaram fotografias tão realistas que os humanos são incapazes de perceber que ilustram objetos, cenários ou pessoas que não existem no mundo real. As GANs mostraram grande potencial em várias aplicações, como síntese de imagens e vídeos, síntese de texto para imagem, transferência de estilo e aumento de dados.\n",
        "\n",
        "### Exemplos de Aplicações das GANs:\n",
        "1. **Geração de Imagens Realistas**:  \n",
        "   - Criar rostos, paisagens ou objetos que parecem reais, mas não existem.  \n",
        "   - Exemplo: *This Person Does Not Exist* (Este site gera rostos humanos realistas usando GANs).\n",
        "\n",
        "2. **Tradução de Imagens**:  \n",
        "   - Converter imagens de um domínio para outro, como transformar fotos diurnas em noturnas ou fotos de verão em inverno.  \n",
        "   - Exemplo: *CycleGAN* (usada para tradução de imagens sem pares correspondentes).\n",
        "\n",
        "3. **Transferência de Estilo**:  \n",
        "   - Aplicar o estilo artístico de uma imagem a outra, como transformar uma foto em uma pintura no estilo de Van Gogh.  \n",
        "   - Exemplo: *StyleGAN* (usada para gerar imagens com estilos artísticos específicos).\n",
        "\n",
        "4. **Síntese de Texto para Imagem**:  \n",
        "   - Gerar imagens a partir de descrições textuais, como criar uma imagem de um pássaro azul com base em uma descrição.  \n",
        "   - Exemplo: *StackGAN* (usada para gerar imagens realistas a partir de texto).\n",
        "\n",
        "5. **Aumento de Dados**:  \n",
        "   - Gerar dados sintéticos para aumentar conjuntos de dados de treinamento, especialmente útil em cenários com dados limitados.  \n",
        "   - Exemplo: Uso em diagnósticos médicos para gerar imagens de raios-X sintéticas.\n",
        "\n",
        "6. **Restauração de Imagens**:  \n",
        "   - Melhorar a qualidade de imagens antigas ou danificadas, como restaurar fotos históricas.  \n",
        "   - Exemplo: *SRGAN* (usada para super-resolução de imagens).\n",
        "\n",
        "7. **Criação de Arte e Design**:  \n",
        "   - Gerar obras de arte, designs de moda ou arquitetura criativos e únicos.  \n",
        "   - Exemplo: *ArtGAN* (usada para criar arte generativa).\n",
        "\n",
        "### Recursos para Leitura Adicional:\n",
        "- **Artigos de Pesquisa**:  \n",
        "  - \"Generative Adversarial Networks\" (Ian Goodfellow et al., 2014) – O artigo original que introduziu as GANs.  \n",
        "  - \"Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks\" (CycleGAN).  \n",
        "  - \"A Style-Based Generator Architecture for Generative Adversarial Networks\" (StyleGAN).  \n",
        "\n",
        "- **Ferramentas e Frameworks**:  \n",
        "  - TensorFlow e PyTorch: Frameworks populares para implementar GANs.  \n",
        "  - Keras: Biblioteca de alto nível para construir e treinar GANs.  \n",
        "\n",
        "As GANs continuam a evoluir e a encontrar novas aplicações em diversos campos, demonstrando seu potencial para revolucionar a maneira como criamos e interagimos com dados visuais e além.\n"
      ],
      "metadata": {
        "id": "bMyEKuM8eifW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Links de acesso às utilizações das GANs:\n",
        "\n",
        "1. **Geração de Imagens Realistas**:\n",
        "   - **This Person Does Not Exist**: [https://thispersondoesnotexist.com/](https://thispersondoesnotexist.com/)\n",
        "   - **Artbreeder**: [https://www.artbreeder.com/](https://www.artbreeder.com/)\n",
        "\n",
        "2. **Tradução de Imagens**:\n",
        "   - **CycleGAN**: [https://junyanz.github.io/CycleGAN/](https://junyanz.github.io/CycleGAN/)\n",
        "   - **Pix2Pix**: [https://phillipi.github.io/pix2pix/](https://phillipi.github.io/pix2pix/)\n",
        "\n",
        "3. **Transferência de Estilo**:\n",
        "   - **DeepArt**: [https://deepart.io/](https://deepart.io/)\n",
        "   - **Neural Style Transfer**: [https://www.tensorflow.org/tutorials/generative/style_transfer](https://www.tensorflow.org/tutorials/generative/style_transfer)\n",
        "\n",
        "4. **Síntese de Texto para Imagem**:\n",
        "   - **StackGAN**: [https://github.com/hanzhanggit/StackGAN](https://github.com/hanzhanggit/StackGAN)\n",
        "   - **AttnGAN**: [https://github.com/taoxugit/AttnGAN](https://github.com/taoxugit/AttnGAN)\n",
        "\n",
        "5. **Aumento de Dados**:\n",
        "   - **Data Augmentation with GANs**: [https://arxiv.org/abs/1711.04340](https://arxiv.org/abs/1711.04340)\n",
        "   - **Medical Image Synthesis with GANs**: [https://arxiv.org/abs/1809.06222](https://arxiv.org/abs/1809.06222)\n",
        "\n",
        "6. **Restauração de Imagens**:\n",
        "   - **SRGAN**: [https://arxiv.org/abs/1609.04802](https://arxiv.org/abs/1609.04802)\n",
        "   - **DeOldify**: [https://github.com/jantic/DeOldify](https://github.com/jantic/DeOldify)\n",
        "\n",
        "7. **Criação de Arte e Design**:\n",
        "   - **ArtGAN**: [https://arxiv.org/abs/1708.09533](https://arxiv.org/abs/1708.09533)\n",
        "   - **Runway ML**: [https://runwayml.com/](https://runwayml.com/)\n",
        "\n",
        "Esses links fornecem uma variedade de exemplos práticos e recursos adicionais para explorar as aplicações das GANs. Eles incluem desde demonstrações interativas até repositórios de código e artigos de pesquisa."
      ],
      "metadata": {
        "id": "qgLa4h_CUiqC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Síntese de Imagens e Vídeos\n",
        "As GANs têm sido amplamente utilizadas na síntese de imagens e vídeos, onde geram imagens ou vídeos realistas que se assemelham a dados do mundo real. Por exemplo, as GANs foram usadas para criar imagens realistas de rostos, que podem ser aplicadas em realidade virtual e visão computacional.\n",
        "\n",
        "### Transferência de Estilo\n",
        "As GANs também têm sido usadas para transferência de estilo, onde o estilo de uma imagem é transferido para outra. Essa aplicação pode ser usada para fins artísticos, bem como nos campos de moda e design.\n",
        "\n",
        "### Aumento de Dados\n",
        "As GANs têm sido utilizadas para aumento de dados em tarefas de aprendizado de máquina, onde podem gerar novos dados de treinamento a partir de dados existentes. Isso pode ajudar a melhorar a precisão dos modelos de aprendizado de máquina.\n",
        "\n",
        "### Análise de Imagens Médicas\n",
        "As GANs têm sido aplicadas na análise de imagens médicas, onde podem gerar imagens sintéticas que podem ser usadas para aumentar os dados de treinamento. Isso pode ajudar a melhorar a precisão dos algoritmos de análise de imagens médicas.\n",
        "\n",
        "### Geração de Objetos 3D\n",
        "As GANs têm sido usadas para gerar objetos 3D, que podem ser aplicados em realidade virtual e outras áreas.\n",
        "\n",
        "### Síntese de Fala\n",
        "As GANs também têm sido utilizadas para síntese de fala, onde podem gerar fala que soa como uma voz humana. Essa aplicação tem muitos usos potenciais, incluindo assistentes de voz e chatbots.\n",
        "\n",
        "Esses são apenas alguns exemplos das muitas aplicações potenciais das GANs, e seu uso ainda está sendo explorado em vários campos. A figura abaixo ilustra algumas dessas aplicações."
      ],
      "metadata": {
        "id": "9gUU88oi44ek"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center><img src='https://drive.google.com/uc?id=13tLfbiOmuCf94gIlamkMQPUlY2lTZfyU' width=800></center>"
      ],
      "metadata": {
        "id": "gZB2cooK6OKp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###links e recursos com exemplos das aplicações das GANs mencionadas:\n",
        "\n",
        "1. **Síntese de Imagens e Vídeos**:\n",
        "   - **This Person Does Not Exist**: [https://thispersondoesnotexist.com/](https://thispersondoesnotexist.com/)\n",
        "   - **DeepFakes**: [https://github.com/iperov/DeepFaceLab](https://github.com/iperov/DeepFaceLab)\n",
        "\n",
        "2. **Transferência de Estilo**:\n",
        "   - **DeepArt**: [https://deepart.io/](https://deepart.io/)\n",
        "   - **Neural Style Transfer**: [https://www.tensorflow.org/tutorials/generative/style_transfer](https://www.tensorflow.org/tutorials/generative/style_transfer)\n",
        "\n",
        "3. **Aumento de Dados**:\n",
        "   - **Data Augmentation with GANs**: [https://arxiv.org/abs/1711.04340](https://arxiv.org/abs/1711.04340)\n",
        "   - **Medical Image Synthesis with GANs**: [https://arxiv.org/abs/1809.06222](https://arxiv.org/abs/1809.06222)\n",
        "\n",
        "4. **Análise de Imagens Médicas**:\n",
        "   - **Medical Image Synthesis with GANs**: [https://arxiv.org/abs/1809.06222](https://arxiv.org/abs/1809.06222)\n",
        "   - **GANs for Medical Imaging**: [https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7325784/](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7325784/)\n",
        "\n",
        "5. **Geração de Objetos 3D**:\n",
        "   - **3D-GAN**: [https://arxiv.org/abs/1610.07584](https://arxiv.org/abs/1610.07584)\n",
        "   - **ShapeNet**: [https://shapenet.org/](https://shapenet.org/)\n",
        "\n",
        "6. **Síntese de Fala**:\n",
        "   - **WaveGAN**: [https://arxiv.org/abs/1802.04208](https://arxiv.org/abs/1802.04208)\n",
        "   - **GAN-TTS**: [https://arxiv.org/abs/1909.11646](https://arxiv.org/abs/1909.11646)\n",
        "\n",
        "Esses links fornecem uma variedade de exemplos práticos e recursos adicionais para explorar as aplicações das GANs. Eles incluem desde demonstrações interativas até repositórios de código e artigos de pesquisa."
      ],
      "metadata": {
        "id": "yX7iyveXVdyW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Síntese de Texto para Imagem  \n",
        "As GANs também têm sido usadas para gerar imagens a partir de descrições textuais. Dada uma descrição textual, uma GAN pode gerar uma imagem que corresponde a essa descrição. Essa aplicação tem muitos usos potenciais, incluindo nos campos de design e arte.\n",
        "\n",
        "Ao usar GANs para síntese de imagens, a capacidade de controlar o conteúdo das imagens geradas é desejada. A síntese de texto para imagem é uma das técnicas mais buscadas em visão computacional, pois, se um algoritmo pode gerar imagens realistas apenas a partir de descrições textuais, isso indica que o algoritmo entende o conteúdo das imagens.\n",
        "\n",
        "A síntese de imagens de alta qualidade a partir de descrições textuais é um problema desafiador em visão computacional. Amostras geradas por abordagens existentes de texto para imagem podem refletir aproximadamente o significado das descrições dadas, mas falham em conter detalhes necessários e partes vívidas dos objetos.\n",
        "\n",
        "A rede GAN construída para essa aplicação é a **StackGAN** (Generative Adversarial Network Empilhada). [Zhang et al.](http://arxiv.org/abs/1612.03242) conseguiram gerar imagens foto-realísticas de 256 × 256 condicionadas a descrições textuais.\n",
        "\n",
        "### Funcionamento da StackGAN:\n",
        "A StackGAN opera em dois estágios:\n",
        "\n",
        "1. **Estágio I**:  \n",
        "   - A StackGAN esboça a forma primitiva e as cores do objeto com base na descrição textual fornecida.  \n",
        "   - Gera imagens de baixa resolução com formas aproximadas e cores básicas.\n",
        "\n",
        "2. **Estágio II**:  \n",
        "   - A StackGAN recebe a saída do Estágio I e a descrição textual como entrada.  \n",
        "   - Gera imagens de alta resolução com detalhes foto-realísticos.  \n",
        "   - É capaz de corrigir defeitos nas imagens criadas no Estágio I e adicionar detalhes convincentes por meio de um processo de refinamento.\n",
        "\n",
        "### Aplicações:\n",
        "- **Design e Arte**: Criação de imagens a partir de descrições textuais para uso em projetos criativos.  \n",
        "- **Publicidade e Marketing**: Geração de imagens personalizadas com base em descrições de produtos.  \n",
        "- **Educação e Treinamento**: Criação de materiais visuais a partir de descrições textuais para fins educacionais.\n",
        "\n",
        "### Recursos Adicionais:\n",
        "- **Artigo Original da StackGAN**: [StackGAN: Text to Photo-realistic Image Synthesis with Stacked Generative Adversarial Networks](http://arxiv.org/abs/1612.03242)  \n",
        "- **Repositório no GitHub**: [https://github.com/hanzhanggit/StackGAN](https://github.com/hanzhanggit/StackGAN)  \n",
        "\n",
        "Essa técnica demonstra o potencial das GANs para entender e traduzir descrições textuais em imagens visualmente convincentes, abrindo caminho para aplicações inovadoras em diversos campos."
      ],
      "metadata": {
        "id": "xCNDdnEG4qyp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center><img src='https://drive.google.com/uc?id=1E0J7Zr7NPO01PgbrfyaZ_Q4EC1WxBC0c' width=900></center>"
      ],
      "metadata": {
        "id": "Gw7iW8MxgsUU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A imagem acima ilustra o funcionamento da **StackGAN** em duas etapas para gerar imagens a partir de descrições textuais.\n",
        "\n",
        "### Descrição do Conteúdo:\n",
        "1. **Descrições Textuais**:  \n",
        "   - Três descrições textuais são fornecidas:  \n",
        "     - Um pássaro branco com preto na cabeça e asas, e um bico laranja longo.  \n",
        "     - Um pássaro com barriga amarela, tarso amarelo, costas e asas cinza, garganta marrom e nuca com rosto preto.  \n",
        "     - Uma flor com pétalas pontiagudas rosa sobrepostas, rodeadas por um anel de filamentos amarelos curtos.  \n",
        "\n",
        "2. **Imagens Geradas pela StackGAN**:  \n",
        "   - **a) StackGAN Estágio-I**:  \n",
        "     - Gera imagens de baixa resolução (64 × 64 pixels) com formas básicas e cores aproximadas com base nas descrições textuais.  \n",
        "   - **b) StackGAN Estágio-II**:  \n",
        "     - Refina as imagens do Estágio-I, gerando imagens de alta resolução (256 × 256 pixels) com detalhes foto-realísticos.  \n",
        "\n",
        "### Funcionamento da StackGAN:\n",
        "- **Estágio-I**:  \n",
        "  - A StackGAN esboça a forma primitiva e as cores do objeto com base na descrição textual.  \n",
        "  - As imagens geradas são de baixa resolução, mas capturam a essência da descrição.  \n",
        "\n",
        "- **Estágio-II**:  \n",
        "  - A StackGAN recebe a saída do Estágio-I e a descrição textual como entrada.  \n",
        "  - Gera imagens de alta resolução com detalhes realistas, corrigindo defeitos e adicionando nuances.  \n",
        "\n",
        "### Aplicações:\n",
        "- **Design e Arte**: Criação de imagens a partir de descrições textuais para projetos criativos.  \n",
        "- **Publicidade**: Geração de imagens personalizadas com base em descrições de produtos.  \n",
        "- **Educação**: Criação de materiais visuais a partir de descrições textuais para fins educacionais.  \n",
        "\n",
        "### Recursos Adicionais:\n",
        "- **Repositório Oficial da StackGAN**: [https://github.com/hanzhanggit/StackGAN](https://github.com/hanzhanggit/StackGAN)  \n",
        "- **Artigo Original**: [StackGAN: Text to Photo-realistic Image Synthesis](http://arxiv.org/abs/1612.03242)  \n",
        "\n",
        "Essa demonstração ilustra o poder da StackGAN para transformar descrições textuais em imagens visualmente impressionantes, destacando sua utilidade em diversas aplicações."
      ],
      "metadata": {
        "id": "Nq21KRAdWmbF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center><img src='https://drive.google.com/uc?id=10-IILg1DDUkZuA26XQUZCn0SVUBRXDz8' width=800></center>"
      ],
      "metadata": {
        "id": "NGhX53cgGMXs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**StackGAN**: uma arquitetura de GAN usada para síntese de texto para imagem:\n",
        "\n",
        "1. **Repositório Oficial da StackGAN no GitHub**:  \n",
        "   - [https://github.com/hanzhanggit/StackGAN](https://github.com/hanzhanggit/StackGAN)  \n",
        "   - Este repositório contém o código-fonte, instruções de instalação e exemplos de uso da StackGAN. Você pode testar o modelo e gerar imagens a partir de descrições textuais.\n",
        "\n",
        "2. **Artigo Original da StackGAN**:  \n",
        "   - [StackGAN: Text to Photo-realistic Image Synthesis with Stacked Generative Adversarial Networks](http://arxiv.org/abs/1612.03242)  \n",
        "   - O artigo descreve a arquitetura e os resultados da StackGAN, incluindo exemplos de imagens geradas a partir de texto.\n",
        "\n",
        "3. **Demonstrações Interativas**:  \n",
        "   - **AttnGAN (uma evolução da StackGAN)**: [https://github.com/taoxugit/AttnGAN](https://github.com/taoxugit/AttnGAN)  \n",
        "     - O AttnGAN é uma versão aprimorada da StackGAN que utiliza mecanismos de atenção para melhorar a qualidade das imagens geradas. O repositório inclui demonstrações e exemplos.\n",
        "\n",
        "4. **Exemplos de Imagens Geradas**:  \n",
        "   - **Página do Projeto StackGAN**: [https://github.com/hanzhanggit/StackGAN#examples](https://github.com/hanzhanggit/StackGAN#examples)  \n",
        "     - Esta seção do repositório oficial contém exemplos de imagens geradas pela StackGAN a partir de descrições textuais.\n",
        "\n",
        "5. **Vídeos e Tutoriais**:  \n",
        "   - **YouTube - StackGAN Tutorial**: [https://www.youtube.com/results?search_query=StackGAN+tutorial](https://www.youtube.com/results?search_query=StackGAN+tutorial)  \n",
        "     - Pesquise por tutoriais no YouTube para ver demonstrações práticas de como usar a StackGAN.\n",
        "\n",
        "6. **Aplicações Práticas**:  \n",
        "   - **Runway ML**: [https://runwayml.com/](https://runwayml.com/)  \n",
        "     - O Runway ML é uma plataforma que oferece ferramentas para trabalhar com modelos generativos, incluindo GANs. Você pode explorar a StackGAN e outras variações de GANs de forma interativa.\n",
        "\n",
        "Esses links fornecem uma variedade de recursos, desde código-fonte e artigos até demonstrações interativas, para que você possa explorar e experimentar a StackGAN e suas aplicações."
      ],
      "metadata": {
        "id": "TMu5gYVIWMc9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tradução de Imagem para Imagem (Pix2Pix GAN)  \n",
        "A *tradução de imagem para imagem* é definida como a tradução de uma representação de uma cena em outra, dados dados de treinamento suficientes. Essa técnica é inspirada na analogia da tradução de idiomas: assim como uma ideia pode ser expressa em muitos idiomas diferentes, uma cena pode ser representada por uma imagem em tons de cinza, uma imagem RGB, mapas de rótulos semânticos, esboços de bordas, entre outros. Na figura, tarefas de tradução de imagem para imagem são demonstradas em uma variedade de aplicações, como converter rótulos de segmentação de cenas de rua em imagens reais, imagens em tons de cinza em imagens coloridas, esboços de produtos em fotografias de produtos e fotografias diurnas em noturnas.\n",
        "\n",
        "O **Pix2Pix** é um membro da família GAN projetado por [Phillip Isola et al.](http://arxiv.org/abs/1611.07004) em 2016 para tradução de imagem para imagem de propósito geral. A arquitetura da rede Pix2Pix é semelhante ao conceito de GAN: ela consiste em um modelo gerador para gerar novas imagens sintéticas que parecem realistas e um modelo discriminador que classifica as imagens como reais (do conjunto de dados) ou falsas (geradas). O processo de treinamento também é semelhante ao usado para GANs: o modelo discriminador é atualizado diretamente, enquanto o modelo gerador é atualizado por meio do modelo discriminador. Assim, os dois modelos são treinados simultaneamente em um processo adversarial onde o gerador busca enganar melhor o discriminador e o discriminador busca identificar melhor as imagens falsas.\n",
        "\n",
        "### Aplicações do Pix2Pix:\n",
        "1. **Conversão de Esboços em Imagens**: Transformar esboços em imagens realistas, como esboços de roupas em fotografias de moda.\n",
        "2. **Colorização de Imagens**: Converter imagens em tons de cinza em imagens coloridas.\n",
        "3. **Tradução de Rótulos Semânticos**: Gerar imagens realistas a partir de mapas de rótulos semânticos, como cenas de rua.\n",
        "4. **Conversão de Dia para Noite**: Transformar fotografias diurnas em noturnas.\n",
        "\n",
        "### Recursos Adicionais:\n",
        "- **Artigo Original do Pix2Pix**: [Image-to-Image Translation with Conditional Adversarial Networks](http://arxiv.org/abs/1611.07004)  \n",
        "- **Repositório no GitHub**: [https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix)  \n",
        "- **Demonstrações Interativas**: [https://affinelayer.com/pix2pix/](https://affinelayer.com/pix2pix/)  \n",
        "\n",
        "Essa técnica demonstra o poder das GANs para transformar representações visuais de uma forma para outra, abrindo caminho para aplicações criativas e práticas em diversos campos."
      ],
      "metadata": {
        "id": "nAVLJwprg9jd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<figure>\n",
        "<center>\n",
        "<img src='https://drive.google.com/uc?id=1WxtnrArM43glty5PqqQUhCTbxp0CpQs7' />\n",
        "<figcaption>Examples of Pix2Pix applications taken from the original paper.</figcaption></center>\n",
        "</figure>"
      ],
      "metadata": {
        "id": "TIO9ygUciO-l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A imagem acima ilustra exemplos de **tradução de imagem para imagem** usando técnicas como o **Pix2Pix GAN**.\n",
        "\n",
        "### Descrição do Conteúdo:\n",
        "1. **Conversão de Preto e Branco para Colorido**:  \n",
        "   - **Entrada (Input)**: Uma imagem em preto e branco.  \n",
        "   - **Saída (Output)**: A mesma imagem colorizada, gerada por um modelo de tradução de imagem.  \n",
        "\n",
        "2. **Conversão de Bordas para Fotos**:  \n",
        "   - **Entrada (Input)**: Um esboço ou desenho de bordas (por exemplo, um esboço de um sapato ou um edifício).  \n",
        "   - **Saída (Output)**: Uma imagem realista gerada a partir do esboço.  \n",
        "\n",
        "3. **Conversão de Dia para Noite**:  \n",
        "   - **Entrada (Input)**: Uma fotografia tirada durante o dia.  \n",
        "   - **Saída (Output)**: A mesma cena transformada em uma imagem noturna.  \n",
        "\n",
        "### Aplicações do Pix2Pix GAN:\n",
        "- **Colorização de Imagens**: Transformar imagens em preto e branco em imagens coloridas.  \n",
        "- **Geração de Imagens a Partir de Esboços**: Converter desenhos ou esboços em imagens realistas.  \n",
        "- **Tradução de Cenas**: Transformar cenas diurnas em noturnas ou vice-versa.  \n",
        "\n",
        "### Como Funciona o Pix2Pix:\n",
        "- O **Pix2Pix** é uma rede GAN condicional que aprende a mapear uma imagem de entrada para uma imagem de saída com base em pares de exemplos (entrada-saída).  \n",
        "- O **gerador** cria imagens realistas a partir da entrada, enquanto o **discriminador** tenta distinguir entre imagens reais e geradas.  \n",
        "- O processo de treinamento é adversarial, onde o gerador e o discriminador melhoram iterativamente.  \n",
        "\n",
        "Essa técnica é poderosa para aplicações criativas e práticas, como design, fotografia e visão computacional, permitindo a transformação de imagens de uma forma para outra de maneira eficiente e realista."
      ],
      "metadata": {
        "id": "Szo_pc0EXTk4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A ideia inovadora das redes **Pix2Pix** é que elas aprendem uma função de perda adaptada à tarefa e aos dados em questão, o que as torna aplicáveis em uma ampla variedade de configurações. Elas são um tipo de **GAN condicional** (**cGAN**), onde a geração da imagem de saída é condicionada a uma imagem de entrada. O discriminador recebe tanto a imagem de origem quanto a imagem de destino e deve determinar se a imagem de destino é uma transformação plausível da imagem de origem.\n",
        "\n",
        "### Funcionamento do Pix2Pix:\n",
        "1. **Gerador**:  \n",
        "   - O gerador recebe uma imagem de entrada (por exemplo, um esboço ou uma imagem em preto e branco) e gera uma imagem de saída (por exemplo, uma imagem colorida ou uma fotografia realista).  \n",
        "   - O objetivo do gerador é produzir imagens que sejam indistinguíveis das imagens reais.\n",
        "\n",
        "2. **Discriminador**:  \n",
        "   - O discriminador recebe dois pares de imagens: a imagem de origem e a imagem de destino (que pode ser real ou gerada pelo gerador).  \n",
        "   - O discriminador deve determinar se a imagem de destino é uma transformação plausível da imagem de origem.  \n",
        "   - Isso torna o discriminador mais poderoso, pois ele não apenas verifica a qualidade da imagem, mas também a consistência entre a entrada e a saída.\n",
        "\n",
        "3. **Função de Perda**:  \n",
        "   - A função de perda do Pix2Pix é adaptada à tarefa específica, o que permite que a rede aprenda a mapear efetivamente a entrada para a saída desejada.  \n",
        "   - Além da perda adversarial (que faz o gerador competir com o discriminador), o Pix2Pix também usa uma perda **L1** ou **L2** para garantir que a imagem gerada seja semelhante à imagem de destino real.\n",
        "\n",
        "### Aplicações do Pix2Pix:\n",
        "- **Colorização de Imagens**: Converter imagens em preto e branco em imagens coloridas.  \n",
        "- **Geração de Imagens a Partir de Esboços**: Transformar desenhos ou esboços em imagens realistas.  \n",
        "- **Tradução de Cenas**: Converter cenas diurnas em noturnas ou vice-versa.  \n",
        "- **Restauração de Imagens**: Melhorar a qualidade de imagens antigas ou danificadas.  \n",
        "\n",
        "### Vantagens do Pix2Pix:\n",
        "- **Flexibilidade**: Pode ser aplicado a uma ampla variedade de tarefas de tradução de imagem para imagem.  \n",
        "- **Qualidade**: Produz imagens de alta qualidade que são consistentes com a entrada.  \n",
        "- **Aprendizado Adaptativo**: A função de perda é aprendida automaticamente, adaptando-se aos dados e à tarefa.  \n",
        "\n"
      ],
      "metadata": {
        "id": "TxSBumOaitfc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GAN de Super-Resolução de Imagem (SRGAN)  \n",
        "Um tipo específico de modelos GAN pode ser usado para converter imagens de baixa resolução em imagens de alta resolução. Esse tipo é chamado de **Super-Resolution Generative Adversarial Networks (SRGAN)** e foi introduzido por [Christian Ledig et al.](http://arxiv.org/abs/1609.04802) em 2016. A figura mostra como o SRGAN foi capaz de criar uma imagem de altíssima resolução.\n",
        "\n",
        "### Funcionamento do SRGAN:\n",
        "1. **Objetivo**:  \n",
        "   - O SRGAN foi projetado para melhorar a resolução de imagens, transformando imagens de baixa resolução em versões de alta resolução que parecem naturais e detalhadas.  \n",
        "\n",
        "2. **Arquitetura**:  \n",
        "   - **Gerador**: O gerador recebe uma imagem de baixa resolução e gera uma versão de alta resolução. Ele é treinado para produzir imagens que sejam indistinguíveis de imagens reais de alta resolução.  \n",
        "   - **Discriminador**: O discriminador recebe tanto imagens reais de alta resolução quanto imagens geradas pelo gerador. Ele deve determinar se a imagem é real ou gerada.  \n",
        "\n",
        "3. **Função de Perda**:  \n",
        "   - O SRGAN usa uma combinação de perda adversarial e perda de conteúdo (baseada em características extraídas por uma rede neural pré-treinada, como a VGG).  \n",
        "   - A perda adversarial ajuda a gerar imagens realistas, enquanto a perda de conteúdo garante que a imagem gerada mantenha os detalhes estruturais da imagem original.  \n",
        "\n",
        "### Aplicações do SRGAN:\n",
        "- **Melhoria de Qualidade de Imagens**: Aumentar a resolução de imagens para melhorar a clareza e os detalhes.  \n",
        "- **Fotografia e Vídeo**: Melhorar a qualidade de fotos e vídeos antigos ou de baixa resolução.  \n",
        "- **Imagens Médicas**: Aumentar a resolução de imagens médicas, como raios-X e ressonâncias magnéticas, para melhorar a análise.  \n",
        "- **Reconhecimento de Imagens**: Melhorar a precisão de algoritmos de reconhecimento de imagens, fornecendo imagens de alta resolução como entrada.  \n",
        "\n",
        "### Vantagens do SRGAN:\n",
        "- **Qualidade Visual**: Produz imagens de alta resolução com detalhes realistas e naturais.  \n",
        "- **Aprendizado Profundo**: Utiliza redes neurais profundas para aprender a mapear imagens de baixa para alta resolução.  \n",
        "- **Flexibilidade**: Pode ser aplicado a uma variedade de tipos de imagens e domínios.  \n",
        "\n",
        "### Recursos Adicionais:\n",
        "- **Artigo Original do SRGAN**: [Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network](http://arxiv.org/abs/1609.04802)  \n",
        "- **Repositório no GitHub**: [https://github.com/tensorlayer/srgan](https://github.com/tensorlayer/srgan)  \n",
        "- **Demonstrações e Exemplos**: [https://arxiv.org/abs/1609.04802](https://arxiv.org/abs/1609.04802)  \n",
        "\n",
        "O SRGAN é uma ferramenta poderosa para melhorar a resolução de imagens, com aplicações em diversos campos, desde fotografia até medicina e visão computacional."
      ],
      "metadata": {
        "id": "VXcENyHLjDn9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center><img src='https://drive.google.com/uc?id=1R4vEoHIRl1TPfiEb60qp2I9k6erxXesa' width=800></center>"
      ],
      "metadata": {
        "id": "McujuHk2japd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GANs Mais Populares  \n",
        "Existem vários tipos de GANs, mas os mais populares são os seguintes, conforme ilustrado na figura abaixo:\n",
        "\n",
        "1. **GAN Tradicional**:  \n",
        "   - A GAN original, proposta por Ian Goodfellow e colegas em 2014, consiste em um gerador e um discriminador que competem entre si. O gerador tenta criar dados realistas, enquanto o discriminador tenta distinguir entre dados reais e falsos.\n",
        "\n",
        "2. **Conditional GAN (cGAN)**:  \n",
        "   - Uma extensão da GAN tradicional onde tanto o gerador quanto o discriminador recebem informações adicionais, como rótulos ou condições. Isso permite a geração de dados condicionados a entradas específicas.\n",
        "\n",
        "3. **Deep Convolutional GAN (DCGAN)**:  \n",
        "   - Utiliza camadas convolucionais no gerador e no discriminador, o que melhora a qualidade das imagens geradas. A DCGAN é conhecida por sua estabilidade e eficácia em tarefas de geração de imagens.\n",
        "\n",
        "4. **CycleGAN**:  \n",
        "   - Projetada para tradução de imagem para imagem sem a necessidade de pares de treinamento correspondentes. É amplamente usada para tarefas como conversão de estilo, colorização de imagens e transformação de cenas.\n",
        "\n",
        "5. **Wasserstein GAN (WGAN)**:  \n",
        "   - Introduz a função de perda de Wasserstein, que melhora a estabilidade do treinamento e a qualidade das imagens geradas. A WGAN é conhecida por resolver problemas como colapso de modos e instabilidade no treinamento.\n",
        "\n",
        "6. **Progressive GAN (ProGAN)**:  \n",
        "   - Treina o gerador e o discriminador de forma progressiva, começando com imagens de baixa resolução e aumentando gradualmente a resolução. Isso resulta em imagens de alta qualidade e alta resolução.\n",
        "\n",
        "7. **StyleGAN**:  \n",
        "   - Uma evolução da ProGAN que permite controle detalhado sobre o estilo e as características das imagens geradas. A StyleGAN é conhecida por gerar rostos humanos extremamente realistas.\n",
        "\n",
        "8. **Super-Resolution GAN (SRGAN)**:  \n",
        "   - Projetada para aumentar a resolução de imagens, transformando imagens de baixa resolução em versões de alta resolução com detalhes realistas.\n",
        "\n",
        "9. **BigGAN**:  \n",
        "   - Uma GAN de grande escala que gera imagens de alta qualidade em grandes conjuntos de dados. A BigGAN é conhecida por sua capacidade de gerar imagens diversificadas e detalhadas.\n",
        "\n",
        "10. **InfoGAN**:  \n",
        "    - Uma variação da GAN que aprende representações desembaraçadas dos dados, permitindo o controle sobre características específicas das imagens geradas.\n",
        "\n",
        "Esses são alguns dos tipos mais populares de GANs, cada um com suas próprias características e aplicações específicas. A escolha do tipo de GAN depende da tarefa e dos dados em questão."
      ],
      "metadata": {
        "id": "0MFei9mj8NGi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center><img src='https://drive.google.com/uc?id=1diTAn3ji5ZIxdiDsRGuSQOlqIBSNhYZY' width=700></center>"
      ],
      "metadata": {
        "id": "vPVpbwYs8amJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GANs Convolucionais Profundas (DCGANs)  \n",
        "Na GAN original (2014), redes *multilayer perceptron* (MLP) foram usadas para construir as redes do gerador e do discriminador. No entanto, desde então, foi comprovado que camadas convolucionais fornecem maior poder preditivo ao discriminador, o que, por sua vez, melhora a precisão do gerador e do modelo geral. Esse tipo de GAN é chamado de **Deep Convolutional GAN (DCGAN)** e foi desenvolvido por Alec Radford et al. em 2016. Atualmente, todas as arquiteturas de GAN contêm camadas convolucionais, então o \"DC\" está implícito quando falamos de GANs.\n",
        "\n",
        "### Arquitetura do DCGAN:\n",
        "O núcleo da arquitetura **Deep Convolutional Generative Adversarial Network (DCGAN)** é baseado em uma arquitetura padrão de Rede Neural Convolucional (CNN) para um modelo discriminativo. Uma rede neural convolucional utiliza filtros que deslizam sobre os dados de entrada, produzindo ativações usando um conjunto de parâmetros. No DCGAN, cada camada no gerador serve como um mapeamento de um vetor oculto de baixa dimensão para uma imagem de alta dimensão.\n",
        "\n",
        "#### Componentes Principais:\n",
        "1. **Gerador**:  \n",
        "   - Começa com um vetor de ruído aleatório como entrada.  \n",
        "   - Usa camadas convolucionais para \"desconvoluir\" o ruído e gerar dados falsos (imagens).  \n",
        "   - A arquitetura do gerador é projetada para transformar o ruído em uma imagem realista.  \n",
        "\n",
        "2. **Discriminador**:  \n",
        "   - Avalia as imagens geradas pelo gerador e as compara com imagens reais.  \n",
        "   - Usa camadas convolucionais para extrair características das imagens e classificá-las como reais ou falsas.  \n",
        "\n",
        "#### Inovações do DCGAN:\n",
        "- **Substituição de Camadas de Pooling**:  \n",
        "  - Camadas de pooling são substituídas por convoluções com *strides* (passos), o que permite um aprendizado mais eficiente e evita a perda de informações.  \n",
        "\n",
        "- **Normalização em Lotes (Batch Normalization)**:  \n",
        "  - Tanto o gerador quanto o discriminador usam normalização em lotes para estabilizar o treinamento e melhorar a convergência.  \n",
        "\n",
        "- **Camadas Convolucionais em vez de Camadas Totalmente Conectadas**:  \n",
        "  - O uso de camadas convolucionais em vez de camadas totalmente conectadas permite uma arquitetura mais profunda e eficiente.  \n",
        "\n",
        "### Vantagens do DCGAN:\n",
        "- **Melhor Qualidade de Imagens**: As camadas convolucionais permitem a geração de imagens mais detalhadas e realistas.  \n",
        "- **Estabilidade no Treinamento**: A normalização em lotes e a substituição de camadas de pooling ajudam a evitar problemas como colapso de modos e instabilidade.  \n",
        "- **Arquitetura Profunda**: O uso de convoluções permite a construção de redes mais profundas e complexas.  \n",
        "\n",
        "### Aplicações do DCGAN:\n",
        "- **Geração de Imagens**: Criar imagens realistas a partir de ruído aleatório.  \n",
        "- **Aumento de Dados**: Gerar dados sintéticos para aumentar conjuntos de treinamento.  \n",
        "- **Tradução de Imagens**: Transformar imagens de um domínio para outro.  \n",
        "\n",
        "### Recursos Adicionais:\n",
        "- **Artigo Original do DCGAN**: [Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks](https://arxiv.org/abs/1511.06434)  \n",
        "- **Repositório no GitHub**: [https://github.com/Newmu/dcgan_code](https://github.com/Newmu/dcgan_code)  \n",
        "\n",
        "O DCGAN é um marco no desenvolvimento de GANs, introduzindo técnicas que melhoraram significativamente a qualidade e a estabilidade das redes generativas. Sua arquitetura serviu de base para muitas variações modernas de GANs."
      ],
      "metadata": {
        "id": "4qrM4n_jZn0O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GANs Condicionais (cGANs)  \n",
        "A **Conditional GAN (cGAN)** é uma variante da GAN em que o modelo gerador gera imagens de forma condicional, com base em informações adicionais. Essa informação adicional serve como uma restrição sobre os dados que são alimentados no discriminador como dados reais. Ao trabalhar com uma cGAN, é crucial condicionar tanto o gerador quanto o discriminador para que eles estejam cientes do tipo específico de dados com os quais estão lidando. É importante notar que as cGANs podem ser condicionadas em uma ampla gama de entradas. Por exemplo, no contexto de tradução de imagem para imagem, outras imagens podem ser usadas como informações adicionais para condicionar a GAN.\n",
        "\n",
        "### Funcionamento da cGAN:\n",
        "1. **Condicionamento do Gerador**:  \n",
        "   - O gerador recebe não apenas um vetor de ruído aleatório, mas também informações adicionais (como rótulos, classes ou outras imagens).  \n",
        "   - Essas informações condicionais guiam o gerador a produzir imagens específicas que correspondem às condições fornecidas.\n",
        "\n",
        "2. **Condicionamento do Discriminador**:  \n",
        "   - O discriminador também recebe as informações adicionais, além da imagem real ou gerada.  \n",
        "   - Ele deve determinar se a imagem é real ou falsa, levando em consideração as condições fornecidas.\n",
        "\n",
        "3. **Função de Perda**:  \n",
        "   - A função de perda da cGAN é semelhante à da GAN tradicional, mas leva em consideração as informações condicionais.  \n",
        "   - O gerador é penalizado se gerar imagens que não correspondem às condições, e o discriminador é recompensado por identificar corretamente as imagens condicionadas.\n",
        "\n",
        "### Aplicações das cGANs:\n",
        "1. **Tradução de Imagem para Imagem**:  \n",
        "   - Converter imagens de um domínio para outro, como esboços em fotografias ou imagens diurnas em noturnas.  \n",
        "   - Exemplo: **Pix2Pix**, que usa pares de imagens para aprender o mapeamento entre domínios.\n",
        "\n",
        "2. **Geração de Imagens Condicionadas**:  \n",
        "   - Gerar imagens específicas com base em rótulos ou descrições textuais.  \n",
        "   - Exemplo: Gerar imagens de gatos com base em rótulos como \"gato preto\" ou \"gato branco\".\n",
        "\n",
        "3. **Aumento de Dados**:  \n",
        "   - Gerar dados sintéticos condicionados a características específicas, úteis para aumentar conjuntos de treinamento em aprendizado de máquina.\n",
        "\n",
        "4. **Edição de Imagens**:  \n",
        "   - Modificar imagens existentes com base em condições, como adicionar ou remover objetos, alterar cores ou estilos.\n",
        "\n",
        "### Vantagens das cGANs:\n",
        "- **Controle sobre a Saída**: As informações condicionais permitem controlar o tipo de imagem gerada, tornando as cGANs mais versáteis.  \n",
        "- **Aplicações Específicas**: Podem ser adaptadas para tarefas específicas, como tradução de imagem ou geração de imagens com características desejadas.  \n",
        "- **Melhor Qualidade**: O condicionamento ajuda a gerar imagens mais precisas e relevantes para a tarefa em questão.\n",
        "\n",
        "### Recursos Adicionais:\n",
        "- **Artigo Original da cGAN**: [Conditional Generative Adversarial Nets](https://arxiv.org/abs/1411.1784)  \n",
        "- **Repositório no GitHub**: [https://github.com/znxlwm/pytorch-generative-model-collections](https://github.com/znxlwm/pytorch-generative-model-collections)  \n",
        "\n",
        "As cGANs são uma ferramenta poderosa para tarefas que exigem controle sobre a geração de dados, abrindo caminho para aplicações inovadoras em áreas como visão computacional, design e arte generativa."
      ],
      "metadata": {
        "id": "5sQQKClnZ4o9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Redes Adversárias Generativas Cíclicas (CycleGAN)**  \n",
        "\n",
        "O **CycleGAN** é um tipo de GAN utilizado para mapear a distribuição de um domínio de imagem para outro, permitindo a transferência de características entre imagens. Diferentemente de outras abordagens de tradução de imagem para imagem, como o **Pix2Pix**, o CycleGAN não requer pares de imagens correspondentes para o treinamento, tornando-o extremamente versátil para aplicações em que não há correspondência direta entre os domínios de origem e destino.  \n",
        "\n",
        "A arquitetura do CycleGAN é baseada em dois geradores e dois discriminadores:  \n",
        "\n",
        "- O **Gerador $ G $** recebe uma imagem de um domínio $ X $ e a transforma em uma imagem no domínio $ Y $.  \n",
        "- O **Gerador $ F $** recebe a imagem gerada no domínio $ Y $ e tenta reconstruí-la de volta ao domínio original $ X $.  \n",
        "- O **Discriminador $ D_Y $** avalia se uma imagem pertence ao domínio $ Y $ ou se foi gerada pelo $ G $.  \n",
        "- O **Discriminador $ D_X $** avalia se uma imagem pertence ao domínio $ X $ ou se foi gerada pelo $ F $.  \n",
        "\n",
        "O treinamento do CycleGAN é guiado por duas perdas principais:  \n",
        "\n",
        "1. **Perda adversarial**: Garante que as imagens geradas sejam indistinguíveis das imagens reais do domínio de destino.  \n",
        "2. **Perda cíclica (Cycle Consistency Loss)**: Garante que, ao converter uma imagem de $ X $ para $ Y $ e depois de volta para $ X $, a imagem resultante seja semelhante à original.  \n",
        "\n",
        "Matematicamente, a perda cíclica pode ser expressa como:  \n",
        "\n",
        "$$\n",
        "\\mathcal{L}_{cycle} = \\mathbb{E}_{x \\sim p_{data}(x)} [||F(G(x)) - x||_1] + \\mathbb{E}_{y \\sim p_{data}(y)} [||G(F(y)) - y||_1]\n",
        "$$\n",
        "\n",
        "Essa regularização força os geradores a preservar as principais características da imagem original durante a conversão entre domínios, evitando transformações irreversíveis ou distorções excessivas.  \n",
        "\n",
        "---\n",
        "\n",
        "### **Exemplo Prático: Implementação de CycleGAN com TensorFlow e Keras**  \n",
        "\n",
        "Aqui está uma implementação simplificada de um gerador do CycleGAN usando **camadas de convolução transpostas** para aumentar a resolução das imagens geradas.  \n",
        "\n",
        "```python\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "def build_resnet_generator(input_shape=(256, 256, 3)):\n",
        "    inputs = layers.Input(shape=input_shape)\n",
        "\n",
        "    # Primeira camada convolucional\n",
        "    x = layers.Conv2D(64, (7, 7), padding=\"same\")(inputs)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.ReLU()(x)\n",
        "\n",
        "    # Downsampling\n",
        "    x = layers.Conv2D(128, (3, 3), strides=2, padding=\"same\")(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.ReLU()(x)\n",
        "\n",
        "    # Camadas Residuais (ResNet Blocks)\n",
        "    for _ in range(6):\n",
        "        res = layers.Conv2D(128, (3, 3), padding=\"same\")(x)\n",
        "        res = layers.BatchNormalization()(res)\n",
        "        res = layers.ReLU()(res)\n",
        "        res = layers.Conv2D(128, (3, 3), padding=\"same\")(res)\n",
        "        res = layers.BatchNormalization()(res)\n",
        "        x = layers.Add()([x, res])  # Conexão residual\n",
        "\n",
        "    # Upsampling (aumentando a resolução)\n",
        "    x = layers.Conv2DTranspose(64, (3, 3), strides=2, padding=\"same\")(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.ReLU()(x)\n",
        "\n",
        "    # Camada final para reconstrução da imagem\n",
        "    outputs = layers.Conv2D(3, (7, 7), padding=\"same\", activation=\"tanh\")(x)\n",
        "\n",
        "    return tf.keras.Model(inputs, outputs, name=\"CycleGAN_Generator\")\n",
        "\n",
        "# Criando o gerador\n",
        "generator = build_resnet_generator()\n",
        "generator.summary()\n",
        "```\n",
        "\n",
        "#### **Explicação do Código**  \n",
        "\n",
        "1. **Entrada da Rede:** O gerador recebe uma imagem de entrada com tamanho $ 256 \\times 256 $ e 3 canais de cor (RGB).  \n",
        "2. **Primeira Camada Convolucional:** Aplica um filtro $ 7 \\times 7 $ para capturar padrões iniciais da imagem.  \n",
        "3. **Downsampling:** Reduz a resolução da imagem para aprender representações mais abstratas.  \n",
        "4. **Blocos Residuais:** Melhoram a capacidade do modelo de capturar detalhes sutis da imagem sem perder informações importantes.  \n",
        "5. **Upsampling:** Restaura a imagem para sua resolução original.  \n",
        "6. **Camada Final:** Usa a ativação **tanh** para manter os valores da imagem no intervalo [-1, 1], facilitando o treinamento adversarial.  \n",
        "\n",
        "Essa arquitetura permite que o CycleGAN aprenda a mapear um domínio para outro sem a necessidade de pares de imagens correspondentes no conjunto de treinamento.  "
      ],
      "metadata": {
        "id": "ugvmplsgasml"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Semi-supervised Generative Adversarial Network (SGAN)\n",
        "Semi-supervised learning represents a promising and highly valuable domain for the\n",
        "practical implementation of GANs. The objective of semi-supervised learning is to learn\n",
        "from a limited number of labeled data points and successfully classify unlabeled data by\n",
        "leveraging the latent structures present in the data. It is essential for the labeled and unlabeled data to be derived from the same underlying distribution to ensure the effectiveness of semi-supervised learning.\n",
        "\n",
        "###Least Squares Generative Adversarial Network (LSGAN)\n",
        "Unsupervised learning using GANs has shown remarkable effectiveness. In traditional\n",
        "GANs, the discriminator is typically treated as a classifier, and the sigmoid cross entropy loss function is used to model its behavior. However, research has revealed that using this loss function exclusively throughout the learning process can lead to vanishing gradients, which hamper effective training. To address this challenge, the use of least squares GANs (LSGANs) has emerged as a beneficial alternative to standard GANs. LSGANs offer two notable advantages. Firstly, the generated images produced by LSGANs tend to be of higher quality compared to those generated by standard GANs. Secondly, LSGANs demonstrate more stable performance throughout the learning process, resulting in improved\n",
        "stability and convergence.\n",
        "\n",
        "###Wasserstein Generative Adversarial Networks (WGAN)\n",
        "This specific GAN tackles several critical aspects, including enhancing learning stability, mitigating issues, such as mode collapse, and providing meaningful learning curves for effective debugging and hyperparameter optimization. It showcases successful solutions to the associated optimization problem and presents a substantial body of theoretical work that underscores the significant relationships between different distribution distances.\n",
        "\n",
        "###Auxiliary Classifier Generative Adversarial Networks (AC-GAN)\n",
        "In recent years, the use of image-based methodologies and treating executables as\n",
        "images have gained popularity in the field of malware research. In this context, some researchers have employed AC-GAN to generate synthetic malware samples and evaluate the performance of various classification strategies on these generated samples.\n",
        "While achieving good results in distinguishing between real and fake samples, the\n",
        "resulting multiclass classification problem proves to be challenging, according to their\n",
        "findings. Although the AC-GAN-generated images often exhibit a high degree of similarity to actual malware images, the researchers concluded that the AC-GAN-generated\n",
        "samples do not reach the level of “deep fake” malware images in terms of deep learning.\n",
        "\n",
        "###Stacked Generative Adversarial Networks (StackGAN)\n",
        "The described GAN is composed of a hierarchical stack of GANs, each of which is trained\n",
        "to generate lower-level representations based on higher-level representations. At each\n",
        "level of the feature hierarchy, a representation discriminator is implemented to encourage\n",
        "the alignment of the generator’s representation manifold with that of the bottom-up discriminative network. This approach allows the generative model to leverage powerful and\n",
        "biased representations to guide its generation process. In addition to the representation\n",
        "discriminator, the GAN incorporates a conditional loss that promotes the utilization of\n",
        "dependent information from the layer above. It also introduces an entropy loss that maximizes a variational lower bound on the conditional entropy of the generator output. These losses act as incentives for the generator to make effective use of conditional information. The training process involves training each stack of GANs separately before training the entire model from start to finish. Unlike the original GAN, which represents all variations with a single noise vector, this hierarchical GAN divides variations into multiple levels, gradually resolving uncertainties in the top-down generation process.\n",
        "\n",
        "###Progressive, Growing Generative Adversarial Networks (PGAN)\n",
        "The utilization of GANs in generating metasurfaces from a training set of high-performance\n",
        "device layouts holds the potential to significantly reduce the computational requirements\n",
        "for metasurface design. This could lead to substantial cost savings. However, traditional\n",
        "GAN designs may struggle to capture the intricate properties of complex metasurfaces\n",
        "accurately. Consequently, additional computationally expensive design modifications are\n",
        "necessary for the produced devices to function properly. The research demonstrates that\n",
        "by progressively expanding both the network architecture and the training set, GANs\n",
        "can effectively learn spatially fine features from high-resolution training data. This\n",
        "approach proves more effective in creating high-quality devices compared to traditional\n",
        "gradient-based topology optimization methods. As a result, further design refinements\n",
        "are no longer necessary to achieve optimal device performance."
      ],
      "metadata": {
        "id": "6e91yQmQR5mU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Redes Adversárias Generativas Ampliadas (BigGAN)**  \n",
        "\n",
        "O **BigGAN** é uma variante especializada das GANs projetada para gerar imagens de **alta resolução** e **alta fidelidade**. Ele incorpora diversas melhorias em relação às arquiteturas GAN tradicionais, permitindo a síntese de imagens realistas em resoluções muito maiores do que as alcançadas anteriormente.  \n",
        "\n",
        "O diferencial do BigGAN reside em sua escalabilidade: ele amplia significativamente o tamanho dos modelos, aumentando o número de filtros por camada, a complexidade da rede e o tamanho dos lotes de treinamento (*batch size*). Essas mudanças permitem que o modelo aproveite melhor a capacidade computacional disponível, resultando em imagens com qualidade muito superior.  \n",
        "\n",
        "### **Principais Melhorias do BigGAN**  \n",
        "\n",
        "O BigGAN introduz diversas inovações para aumentar a qualidade das imagens geradas:  \n",
        "\n",
        "1. **Uso de Self-Attention GAN (SAGAN) como base:**  \n",
        "   - Em vez de confiar apenas em convoluções, o BigGAN incorpora **mecanismos de autoatenção**, permitindo que o modelo capture relações de longo alcance na imagem.  \n",
        "   - Isso melhora a consistência estrutural das imagens geradas, pois elementos distantes podem ser correlacionados com mais precisão.  \n",
        "\n",
        "2. **Normalização Espectral em $ G $ e $ D $:**  \n",
        "   - **Spectral Normalization** é aplicada tanto ao **Generator (G)** quanto ao **Discriminator (D)** para estabilizar o treinamento e evitar gradientes explosivos.  \n",
        "   - Essa técnica reduz a sensibilidade da rede a variações súbitas nos pesos, garantindo que as atualizações sejam mais controladas.  \n",
        "\n",
        "3. **Regra de Atualização em Duas Escalas (Two TimeScale Update Rule - TTUR):**  \n",
        "   - Define taxas de aprendizado **separadas** para o **Generator** e o **Discriminator**, melhorando a estabilidade do treinamento.  \n",
        "   - Isso evita que um modelo aprenda muito mais rápido que o outro, um problema comum em GANs.  \n",
        "\n",
        "4. **Função de perda Hinge Loss:**  \n",
        "   - Em vez de usar a **Binary Cross-Entropy Loss**, o BigGAN adota a **Hinge Loss**, que melhora a qualidade da imagem gerada e a estabilidade do treinamento.  \n",
        "   - A Hinge Loss é definida como:  \n",
        "     \n",
        "     $$\n",
        "     L_D = \\mathbb{E}_{x \\sim p_{data}} [\\max(0, 1 - D(x))] + \\mathbb{E}_{z \\sim p_z} [\\max(0, 1 + D(G(z)))]\n",
        "     $$\n",
        "\n",
        "5. **Batch Normalization Condicional (Class-Conditional Batch Normalization):**  \n",
        "   - Em vez de um **Multi-Layer Perceptron (MLP)** tradicional para condicionar a geração de imagens, o BigGAN usa uma **projeção linear** para incluir informações de classe no Generator.  \n",
        "   - Isso permite que o modelo gere imagens de categorias específicas, tornando-o **condicional**.  \n",
        "\n",
        "6. **Projeção de classe no Discriminador (Projection Discriminator):**  \n",
        "   - O **Discriminator** recebe informações sobre a classe da imagem e avalia se a amostra pertence à categoria correta.  \n",
        "   - Essa abordagem melhora a qualidade das imagens geradas, garantindo que estejam alinhadas com a classe esperada.  \n",
        "\n",
        "7. **Atualização Exponencialmente Ponderada dos Pesos (Exponentially Weighted Moving Average - EWMA):**  \n",
        "   - Similar ao **Progressive GAN (ProGAN)**, essa técnica suaviza os pesos do Generator ao longo do treinamento, ajudando a evitar mudanças bruscas e tornando o modelo mais estável.  \n",
        "\n",
        "---\n",
        "\n",
        "### **Inovações do BigGAN**  \n",
        "\n",
        "Além das melhorias acima, o BigGAN também implementa técnicas inovadoras para maximizar o desempenho:  \n",
        "\n",
        "- **Aumento do tamanho do batch:**  \n",
        "  - Lotes maiores durante o treinamento resultam em maior estabilidade e melhor qualidade de imagem.  \n",
        "  - Isso melhora métricas como o **Inception Score (IS)**, que mede a diversidade e realismo das imagens geradas.  \n",
        "\n",
        "- **Aumento da largura das camadas na rede:**  \n",
        "  - O número de filtros em cada camada da rede foi ampliado, permitindo a captura de mais detalhes na imagem.  \n",
        "\n",
        "- **Adoção de mais conexões de salto (skip connections) entre camadas:**  \n",
        "  - O modelo incorpora conexões diretas entre o **vetor latente** e camadas mais profundas da rede, permitindo que características globais influenciem melhor a geração da imagem.  \n",
        "\n",
        "- **Uso de regularização ortogonal (Orthogonal Regularization):**  \n",
        "  - Técnica que incentiva os pesos da rede a se manterem **ortogonais**, melhorando a estabilidade do treinamento e evitando colapsos no modo de geração (*mode collapse*).  \n",
        "\n",
        "---\n",
        "\n",
        "### **Exemplo Prático: Implementação Simplificada de BigGAN**  \n",
        "\n",
        "Aqui está uma implementação básica de um **Generator** inspirado no BigGAN, usando **normalização espectral** e **batch normalization condicional**.  \n",
        "\n",
        "```python\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "def build_biggan_generator(latent_dim=128, num_classes=10):\n",
        "    noise_input = layers.Input(shape=(latent_dim,))\n",
        "    class_input = layers.Input(shape=(num_classes,))  # Vetor de classe condicional\n",
        "\n",
        "    # Projeção Linear para incorporar classe no ruído\n",
        "    class_projection = layers.Dense(latent_dim, activation=\"linear\")(class_input)\n",
        "    x = layers.Multiply()([noise_input, class_projection])\n",
        "\n",
        "    # Primeira camada totalmente conectada\n",
        "    x = layers.Dense(4 * 4 * 512, use_bias=False)(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.ReLU()\n",
        "    x = layers.Reshape((4, 4, 512))\n",
        "\n",
        "    # Camadas convolucionais transpostas com normalização espectral\n",
        "    x = layers.Conv2DTranspose(256, (4, 4), strides=2, padding=\"same\", use_bias=False)(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.ReLU()\n",
        "\n",
        "    x = layers.Conv2DTranspose(128, (4, 4), strides=2, padding=\"same\", use_bias=False)(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.ReLU()\n",
        "\n",
        "    x = layers.Conv2DTranspose(64, (4, 4), strides=2, padding=\"same\", use_bias=False)(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.ReLU()\n",
        "\n",
        "    # Camada final: converte para uma imagem RGB com valores entre -1 e 1\n",
        "    outputs = layers.Conv2DTranspose(3, (4, 4), strides=2, padding=\"same\", activation=\"tanh\")(x)\n",
        "\n",
        "    return tf.keras.Model([noise_input, class_input], outputs, name=\"BigGAN_Generator\")\n",
        "\n",
        "# Criando o gerador BigGAN\n",
        "generator = build_biggan_generator()\n",
        "generator.summary()\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **Explicação do Código**  \n",
        "\n",
        "1. **Entrada do modelo:**  \n",
        "   - Recebe dois vetores:  \n",
        "     - **Vetor de ruído latente** ($ z $, dimensão **128**)  \n",
        "     - **Vetor de classe** ($ y $, representando a categoria da imagem gerada)  \n",
        "\n",
        "2. **Projeção linear da classe:**  \n",
        "   - O vetor de classe é projetado para a mesma dimensão do ruído latente.  \n",
        "   - O ruído é então multiplicado pela projeção da classe, garantindo que a informação condicional influencie todas as camadas seguintes.  \n",
        "\n",
        "3. **Blocos convolucionais transpostos:**  \n",
        "   - Aplicam **Batch Normalization** e **ReLU** para estabilizar o treinamento.  \n",
        "   - Cada camada dobra a resolução da imagem.  \n",
        "\n",
        "4. **Camada final:**  \n",
        "   - Usa ativação **tanh** para gerar imagens RGB com valores entre **-1 e 1**.  \n",
        "\n",
        "---\n",
        "\n",
        "### **Conclusão**  \n",
        "\n",
        "O BigGAN representa um grande avanço na geração de imagens realistas e de alta resolução. Suas melhorias estruturais e técnicas garantem um desempenho superior em comparação com GANs tradicionais. Com a normalização espectral, autoatenção e lotes maiores, o modelo é capaz de produzir imagens altamente detalhadas e coerentes, sendo amplamente utilizado para gerar imagens fotorrealistas de objetos e paisagens.  "
      ],
      "metadata": {
        "id": "pjQDZYuR_y1S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Style-based Generative Adversarial Networks (StyleGAN)\n",
        "StyleGAN is a GAN that generates fake images in a multi-stage process, starting from\n",
        "a low resolution and gradually progressing to a high resolution of 1024 × 1024 pixels in\n",
        "each iteration. This staged generation allows for independent control over the visual characteristics expressed at each level. By adjusting the input at each level independently, StyleGAN enables control over various visual traits, ranging from broad features like pose and face shape to more specific attributes like hair color. Importantly, these adjustments have no impact on other levels, ensuring the targeted control of specific characteristics. This approach not only provides a deeper understanding of the generated output but also yields impressive results, particularly in generating high-resolution images that appear more realistic and genuine compared to previous methods."
      ],
      "metadata": {
        "id": "AxmJ_E-AAloz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Redes Adversárias Generativas Baseadas em Estilo (StyleGAN)**  \n",
        "\n",
        "O **StyleGAN** é uma abordagem inovadora para a geração de imagens, introduzida pela NVIDIA em 2018. Ele melhora significativamente a qualidade das imagens geradas, permitindo um controle mais refinado sobre diferentes características visuais.  \n",
        "\n",
        "Ao contrário dos **GANs convencionais**, onde o vetor latente é mapeado diretamente para a imagem gerada, o **StyleGAN** introduz um modelo hierárquico de manipulação de estilo. Isso permite que características **globais** (como pose e formato do rosto) e **detalhes finos** (como textura da pele e cor dos olhos) sejam controlados **independentemente**.  \n",
        "\n",
        "---\n",
        "\n",
        "### **Principais Inovações do StyleGAN**  \n",
        "\n",
        "O StyleGAN introduziu várias técnicas revolucionárias para melhorar a qualidade e a flexibilidade da geração de imagens:  \n",
        "\n",
        "### 1. **Arquitetura Progressiva para Alta Resolução**  \n",
        "- O modelo começa com uma **resolução baixa** e, progressivamente, aumenta a resolução até atingir **1024 × 1024 pixels**.  \n",
        "- Isso reduz instabilidades no treinamento e melhora a nitidez dos detalhes finos.  \n",
        "\n",
        "### 2. **Espaço Latente Aprimorado (Z → W)**  \n",
        "- Em GANs tradicionais, o vetor latente **Z** é mapeado diretamente para a imagem final.  \n",
        "- No StyleGAN, um **novo espaço latente W** é aprendido, permitindo maior controle sobre as características geradas.  \n",
        "- Esse novo espaço é menos restritivo e mais alinhado com os fatores perceptivos das imagens reais.  \n",
        "\n",
        "### 3. **Rede de Mapeamento (Mapping Network)**  \n",
        "- Antes que o vetor latente seja passado para o **Generator**, ele é transformado por uma **rede de mapeamento totalmente conectada**.  \n",
        "- Isso ajuda a aprender um espaço latente **desacoplado** dos detalhes visuais da imagem, melhorando a diversidade da geração.  \n",
        "\n",
        "### 4. **Modulação por Estilo (Adaptive Instance Normalization - AdaIN)**  \n",
        "- O vetor latente transformado (**W**) é injetado em diferentes níveis da rede do Generator usando **AdaIN**.  \n",
        "- Isso permite que características **de alto nível** (como formato do rosto) e **de baixo nível** (como textura da pele) sejam ajustadas independentemente.  \n",
        "\n",
        "### 5. **Ruído Estocástico**  \n",
        "- Pequenos detalhes como rugas e texturas da pele são adicionados a partir de um ruído estocástico, tornando a imagem mais realista.  \n",
        "- Diferente do ruído inicial do Generator, esse ruído é **localizado**, afetando apenas certas regiões da imagem.  \n",
        "\n",
        "---\n",
        "\n",
        "## **Controle Hierárquico de Estilo no StyleGAN**  \n",
        "\n",
        "O maior diferencial do StyleGAN é sua capacidade de **controlar características visuais em diferentes escalas**.  \n",
        "\n",
        "| **Nível**         | **Características Controladas**      |\n",
        "|------------------|----------------------------------|\n",
        "| **Baixa Resolução**  | Pose, forma geral do rosto     |\n",
        "| **Média Resolução**  | Estrutura facial, formato do nariz |\n",
        "| **Alta Resolução**   | Textura da pele, cor dos olhos  |\n",
        "\n",
        "Isso significa que **alterar um estilo de alto nível não impacta detalhes de baixo nível**, garantindo controle preciso sobre a imagem gerada.  \n",
        "\n",
        "---\n",
        "\n",
        "## **Exemplo Prático: Implementação Simplificada do StyleGAN**  \n",
        "\n",
        "Abaixo, um exemplo básico de como o **Generator do StyleGAN** pode ser estruturado:  \n",
        "\n",
        "```python\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "def stylegan_generator(latent_dim=512, resolution=1024):\n",
        "    # Entrada: vetor latente (espaço W)\n",
        "    w_input = layers.Input(shape=(latent_dim,))\n",
        "    \n",
        "    # Transformação do vetor latente pelo Mapping Network\n",
        "    w = layers.Dense(512, activation=\"relu\")(w_input)\n",
        "    w = layers.Dense(512, activation=\"relu\")(w)\n",
        "    \n",
        "    # Camada inicial (4x4 pixels)\n",
        "    x = layers.Dense(4 * 4 * 512, activation=\"relu\")(w)\n",
        "    x = layers.Reshape((4, 4, 512))(x)\n",
        "    \n",
        "    # Camadas progressivas de upsampling até 1024x1024\n",
        "    for filters in [256, 128, 64, 32, 16]:  \n",
        "        x = layers.Conv2DTranspose(filters, (3, 3), strides=2, padding=\"same\", activation=\"relu\")(x)\n",
        "        x = layers.BatchNormalization()(x)\n",
        "    \n",
        "    # Camada final - converte para imagem RGB (-1 a 1)\n",
        "    output = layers.Conv2D(3, (3, 3), padding=\"same\", activation=\"tanh\")(x)\n",
        "    \n",
        "    return tf.keras.Model(w_input, output, name=\"StyleGAN_Generator\")\n",
        "\n",
        "# Criando o Generator do StyleGAN\n",
        "generator = stylegan_generator()\n",
        "generator.summary()\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## **Explicação do Código**  \n",
        "\n",
        "1. **Mapping Network:**  \n",
        "   - Transforma o vetor latente **Z** no espaço aprimorado **W**.  \n",
        "   - O novo espaço latente permite controle refinado sobre características visuais.  \n",
        "\n",
        "2. **Camada Inicial (4x4 pixels):**  \n",
        "   - O modelo começa com um **mapa de features muito pequeno** e vai aumentando progressivamente a resolução.  \n",
        "\n",
        "3. **Camadas Progressivas de Upsampling:**  \n",
        "   - Cada camada dobra a resolução, passando por filtros convolucionais e normalização.  \n",
        "   - Isso mantém a coerência dos detalhes na imagem gerada.  \n",
        "\n",
        "4. **Saída Final:**  \n",
        "   - Uma **imagem RGB** é gerada com valores normalizados entre **-1 e 1**.  \n",
        "\n",
        "---\n",
        "\n",
        "## **Resultados do StyleGAN**  \n",
        "\n",
        "O StyleGAN é amplamente utilizado para gerar imagens extremamente realistas. Aqui estão alguns exemplos notáveis de seu uso:  \n",
        "\n",
        "1. **Geração de Rostos Humanos Sintéticos**  \n",
        "   - O site [This Person Does Not Exist](https://thispersondoesnotexist.com/) usa StyleGAN para criar rostos que parecem fotos reais.  \n",
        "\n",
        "2. **Manipulação de Estilo e Interpolação**  \n",
        "   - StyleGAN permite misturar estilos de diferentes imagens, criando transições suaves entre rostos.  \n",
        "\n",
        "3. **Aplicações em Arte Digital e Animação**  \n",
        "   - Artistas usam StyleGAN para gerar retratos sintéticos e estilizar pinturas.  \n",
        "\n",
        "---\n",
        "\n",
        "## **Conclusão**  \n",
        "\n",
        "O **StyleGAN** revolucionou a área de **síntese de imagens**, permitindo um controle sem precedentes sobre a geração. Suas principais vantagens incluem:  \n",
        "\n",
        "✅ **Alta qualidade das imagens geradas**  \n",
        "✅ **Controle preciso sobre diferentes aspectos visuais**  \n",
        "✅ **Treinamento progressivo e mais estável**  \n",
        "✅ **Aplicação em arte, entretenimento e pesquisa científica**  "
      ],
      "metadata": {
        "id": "aMatTLElca-W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Referências\n",
        "[1] https://www.tensorflow.org/tutorials/generative/dcgan?hl=pt-br\n",
        "\n",
        "[2] https://arxiv.org/pdf/1701.00160.pdf\n",
        "\n",
        "[3] https://www.simplilearn.com/tutorials/deep-learning-tutorial/generative-adversarial-networks-gans\n",
        "\n",
        "[4] https://machinelearningmastery.com/what-are-generative-adversarial-networks-gans/\n",
        "\n",
        "[5] https://www.datacamp.com/tutorial/generative-adversarial-networks\n",
        "\n",
        "[6] https://towardsdatascience.com/a-basic-intro-to-gans-generative-adversarial-networks-c62acbcefff3\n",
        "\n",
        "[7] https://www.mygreatlearning.com/blog/generative-adversarial-networks-gans/\n",
        "\n",
        "[8] https://drive.google.com/file/d/1gJkGTTD_apUy1MFvLZZMSnlW7RtLulLJ/view?usp=sharing\n",
        "\n",
        "[9] https://drive.google.com/file/d/1Ia06JK6rffh-cdnhxnaXfiq_f_r-pThO/view?usp=sharing\n",
        "\n",
        "[10] Mehdi Ghayoumi. Generative Adversarial Networks in Practice. CRC Press. 2024."
      ],
      "metadata": {
        "id": "YpQ2B90tj9Tz"
      }
    }
  ]
}